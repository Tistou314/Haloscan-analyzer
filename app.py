"""
Haloscan SEO Diff Analyzer
Version corrig√©e pour le format exact du fichier Baptiste
Avec int√©gration des donn√©es de leads par URL
"""

import streamlit as st
import pandas as pd
import plotly.express as px
from datetime import datetime

st.set_page_config(
    page_title="Haloscan SEO Diff Analyzer",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =============================================================================
# CHARGEMENT DES DONN√âES
# =============================================================================

@st.cache_data
def load_data(uploaded_file):
    """Charge le CSV avec le bon s√©parateur (virgule)"""
    
    # Toujours utiliser la virgule comme s√©parateur
    try:
        df = pd.read_csv(uploaded_file, sep=',', encoding='utf-8')
    except UnicodeDecodeError:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, sep=',', encoding='latin-1')
    
    # Nettoyage des noms de colonnes
    df.columns = (df.columns
        .str.strip()
        .str.lower()
        .str.replace(' ', '_', regex=False)
        .str.replace(';', '', regex=False)
        .str.replace('.', '', regex=False)
        .str.replace('√©', 'e', regex=False)
        .str.replace('√®', 'e', regex=False)
    )
    
    # Mapping vers noms standards
    mapping = {
        'mot-cle_(mc)': 'mot_cle',
        'plus_vieille_pos': 'ancienne_pos',
    }
    df = df.rename(columns=mapping)
    
    # Cr√©er colonne 'volume' √† partir de 'volumeh' si elle n'existe pas
    if 'volume' not in df.columns and 'volumeh' in df.columns:
        df['volume'] = df['volumeh']
    
    # Conversion num√©rique
    for col in ['derniere_pos', 'ancienne_pos', 'meilleure_pos', 'diff_pos', 'volume', 'volumeh', 'trafic', 'cpc']:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Calcul du score de priorit√©
    vol = df['volume'].fillna(0) if 'volume' in df.columns else 0
    diff = df['diff_pos'].fillna(0).abs() if 'diff_pos' in df.columns else 0
    df['priority_score'] = vol * diff
    
    return df


def normalize_url(url):
    """Normalise une URL pour la comparaison"""
    if pd.isna(url):
        return ""
    url = str(url).lower().strip()
    # Retirer le protocole
    url = url.replace('https://', '').replace('http://', '')
    # Retirer www.
    url = url.replace('www.', '')
    # Retirer les doubles slashes (probl√®me fr√©quent)
    while '//' in url:
        url = url.replace('//', '/')
    # Retirer le slash final
    url = url.rstrip('/')
    # Retirer le slash initial si pr√©sent
    url = url.lstrip('/')
    return url

# =============================================================================
# INTERFACE
# =============================================================================

st.title("üìä Haloscan SEO Diff Analyzer")

with st.sidebar:
    st.header("üìÅ Import des donn√©es")
    
    st.subheader("üìä Fichiers Haloscan")
    uploaded_file_p1 = st.file_uploader("1Ô∏è‚É£ CSV Haloscan P√©riode 1", type=['csv'], key="haloscan_p1")
    uploaded_file_p2 = st.file_uploader("2Ô∏è‚É£ CSV Haloscan P√©riode 2", type=['csv'], key="haloscan_p2")
    
    # Labels des p√©riodes (personnalisables)
    if uploaded_file_p1 and uploaded_file_p2:
        st.caption("üìÖ Nommez vos p√©riodes :")
        col1, col2 = st.columns(2)
        with col1:
            label_debut_p1 = st.text_input("D√©but P1", value="Jan 2025", key="label_debut_p1")
            label_fin_p1 = st.text_input("Fin P1 / D√©but P2", value="Sept 2025", key="label_fin_p1")
        with col2:
            label_fin_p2 = st.text_input("Fin P2", value="F√©v 2026", key="label_fin_p2")
    else:
        label_debut_p1 = "D√©but P1"
        label_fin_p1 = "Fin P1"
        label_fin_p2 = "Fin P2"
    
    st.subheader("üí∞ Donn√©es business")
    uploaded_leads = st.file_uploader("3Ô∏è‚É£ Excel Leads par URL (optionnel)", type=['xlsx', 'xls'], 
                                       help="Fichier avec colonnes: url, puis une colonne par mois (YYYY_MM)")

# Variables globales pour les leads
leads_df = None
has_leads = False
month_cols = []
periode_avant = []
periode_apres = []

# Variables pour le mode multi-p√©riodes
has_dual_haloscan = False
df_p1 = None
df_p2 = None

if uploaded_leads:
    # Lire la feuille "Leads totaux par urls" (pas la premi√®re feuille qui contient les visites)
    try:
        xlsx = pd.ExcelFile(uploaded_leads)
        
        # Afficher les feuilles disponibles
        st.sidebar.caption(f"Feuilles : {xlsx.sheet_names}")
        
        # Chercher la feuille des leads par son nom exact ou contenant "lead"
        leads_sheet = None
        for sheet in xlsx.sheet_names:
            if 'lead' in sheet.lower():
                leads_sheet = sheet
                break
        
        if leads_sheet:
            leads_df_raw = pd.read_excel(xlsx, sheet_name=leads_sheet)
            st.sidebar.success(f"üìä Feuille charg√©e : {leads_sheet} ({len(leads_df_raw)} lignes)")
        else:
            # IMPORTANT: La feuille des leads est g√©n√©ralement la 2√®me (index 1)
            # La 1√®re feuille (index 0) contient les visites
            if len(xlsx.sheet_names) > 1:
                leads_df_raw = pd.read_excel(xlsx, sheet_name=1)
                st.sidebar.success(f"üìä Feuille charg√©e : {xlsx.sheet_names[1]} ({len(leads_df_raw)} lignes)")
            else:
                leads_df_raw = pd.read_excel(xlsx, sheet_name=0)
                st.sidebar.warning(f"‚ö†Ô∏è Une seule feuille : {xlsx.sheet_names[0]}")
        
        # V√âRIFICATION : Les leads doivent avoir des valeurs faibles (< 1000 en g√©n√©ral)
        # Si la moyenne est > 500, c'est probablement les visites
        month_cols_check = [c for c in leads_df_raw.columns if '2025' in str(c) or '2024' in str(c)]
        if month_cols_check:
            mean_val = leads_df_raw[month_cols_check].mean().mean()
            if mean_val > 500:
                st.sidebar.error(f"‚ö†Ô∏è ATTENTION : Moyenne = {mean_val:.0f} ‚Äî Ce sont probablement les VISITES, pas les leads !")
                st.sidebar.info("V√©rifiez que la feuille 'Leads totaux par urls' est bien dans le fichier")
            else:
                st.sidebar.info(f"‚úÖ Moyenne = {mean_val:.1f} ‚Äî Donn√©es leads OK")
        
        # Debug : afficher un aper√ßu pour confirmer
        with st.sidebar.expander("üîç V√©rification donn√©es leads", expanded=False):
            st.write(f"Feuilles disponibles : {xlsx.sheet_names}")
            st.write(f"Lignes : {len(leads_df_raw)}")
            # Trouver une colonne de mois pour montrer un exemple
            sample_cols = [c for c in leads_df_raw.columns if '2025' in str(c)][:2]
            if sample_cols and 'url' in leads_df_raw.columns:
                st.write(f"Exemple (premi√®res lignes) :")
                st.dataframe(leads_df_raw[['url'] + sample_cols].head(3))
                
    except Exception as e:
        leads_df_raw = pd.read_excel(uploaded_leads)
        st.sidebar.warning(f"Lecture par d√©faut (erreur: {e})")
    
    # Identifier les colonnes de mois
    month_cols = [col for col in leads_df_raw.columns if col != 'url' and '_' in str(col)]
    month_cols_sorted = sorted(month_cols)
    
    has_leads = True
    
    with st.sidebar:
        st.subheader("üìÖ P√©riodes √† comparer")
        st.caption("S√©lectionnez les mois correspondant √† votre export Haloscan")
        
        # Calculer les valeurs par d√©faut
        default_avant = [c for c in month_cols_sorted if c.startswith('2025_09')]
        if not default_avant:
            default_avant = month_cols_sorted[-6:-3] if len(month_cols_sorted) >= 6 else month_cols_sorted[:3]
        
        default_apres = [c for c in month_cols_sorted if c.startswith('2025_11') or c.startswith('2026')]
        if not default_apres:
            default_apres = month_cols_sorted[-3:] if len(month_cols_sorted) >= 3 else month_cols_sorted[-1:]
        
        # P√©riode AVANT (ancienne position)
        st.markdown("**P√©riode AVANT** (ex: sept 2025)")
        periode_avant = st.multiselect(
            "Mois p√©riode avant",
            options=month_cols_sorted,
            default=default_avant,
            key="avant"
        )
        
        # P√©riode APR√àS (position actuelle)
        st.markdown("**P√©riode APR√àS** (ex: f√©v 2026)")
        periode_apres = st.multiselect(
            "Mois p√©riode apr√®s", 
            options=month_cols_sorted,
            default=default_apres,
            key="apres"
        )
    
    # Calculer les m√©triques leads sur les bonnes p√©riodes
    leads_df = leads_df_raw.copy()
    
    # S'assurer que les colonnes de mois sont num√©riques
    for col in month_cols:
        if col in leads_df.columns:
            leads_df[col] = pd.to_numeric(leads_df[col], errors='coerce').fillna(0)
    
    # Cr√©er les noms de colonnes dynamiques bas√©s sur la s√©lection
    periode_avant_label = '+'.join(periode_avant) if periode_avant else 'N/A'
    periode_apres_label = '+'.join(periode_apres) if periode_apres else 'N/A'
    
    # Calculer les totaux sur TOUS les mois entre le d√©but de p√©riode AVANT et la fin de p√©riode APR√àS
    if periode_avant and periode_apres:
        # Trouver le mois min (d√©but p√©riode) et max (fin p√©riode)
        all_selected = periode_avant + periode_apres
        mois_min = min(all_selected)
        mois_max = max(all_selected)
        
        # Filtrer les colonnes de mois qui sont dans cette plage
        periode_complete = [m for m in month_cols_sorted if mois_min <= m <= mois_max]
        
        if periode_complete:
            leads_df['leads_total'] = leads_df[periode_complete].sum(axis=1)
            st.sidebar.caption(f"üìä Leads total : {mois_min} ‚Üí {mois_max} ({len(periode_complete)} mois)")
        else:
            leads_df['leads_total'] = 0
    elif month_cols:
        leads_df['leads_total'] = leads_df[month_cols].sum(axis=1)
    else:
        leads_df['leads_total'] = 0
        
    leads_df['leads_avant'] = leads_df[periode_avant].sum(axis=1) if periode_avant else 0
    leads_df['leads_apres'] = leads_df[periode_apres].sum(axis=1) if periode_apres else 0
    leads_df['leads_evolution'] = leads_df['leads_apres'] - leads_df['leads_avant']
    leads_df['leads_evolution_pct'] = ((leads_df['leads_apres'] - leads_df['leads_avant']) / leads_df['leads_avant'].replace(0, 1) * 100).round(1)
    
    leads_df['url_normalized'] = leads_df['url'].apply(normalize_url)
    
    st.sidebar.success(f"‚úÖ {len(leads_df):,} URLs avec donn√©es leads")
    if periode_avant and periode_apres:
        st.sidebar.info(f"Comparaison : {periode_avant_label} ‚Üí {periode_apres_label}")

# D√©terminer le mode de fonctionnement
uploaded_file = None
if uploaded_file_p1 and uploaded_file_p2:
    # Mode double p√©riode
    has_dual_haloscan = True
    st.sidebar.success("üìä Mode double p√©riode activ√©")
elif uploaded_file_p1:
    # Mode simple avec P1
    uploaded_file = uploaded_file_p1
elif uploaded_file_p2:
    # Mode simple avec P2
    uploaded_file = uploaded_file_p2

# Charger et fusionner les donn√©es si mode double p√©riode
if has_dual_haloscan:
    df_p1 = load_data(uploaded_file_p1)
    df_p2 = load_data(uploaded_file_p2)
    
    # Renommer les colonnes de position pour P1
    df_p1 = df_p1.rename(columns={
        'ancienne_pos': 'pos_debut_p1',
        'derniere_pos': 'pos_fin_p1',
        'diff_pos': 'diff_p1'
    })
    
    # Renommer les colonnes de position pour P2
    df_p2 = df_p2.rename(columns={
        'ancienne_pos': 'pos_debut_p2',
        'derniere_pos': 'pos_fin_p2',
        'diff_pos': 'diff_p2'
    })
    
    # Fusionner sur mot_cle + url
    df = df_p1.merge(
        df_p2[['mot_cle', 'url', 'pos_debut_p2', 'pos_fin_p2', 'diff_p2']],
        on=['mot_cle', 'url'],
        how='outer',
        suffixes=('', '_p2')
    )
    
    # Calculer les colonnes consolid√©es
    # Position de d√©part = pos_debut_p1 (ou pos_debut_p2 si pas de P1)
    df['ancienne_pos'] = df['pos_debut_p1'].fillna(df['pos_debut_p2'])
    # Position finale = pos_fin_p2 (ou pos_fin_p1 si pas de P2)
    df['derniere_pos'] = df['pos_fin_p2'].fillna(df['pos_fin_p1'])
    # Diff totale : positif = gain (ancienne - derni√®re, car passer de 96 √† 1 = +95)
    df['diff_pos'] = df['ancienne_pos'] - df['derniere_pos']
    
    # Calculer la tendance multi-p√©riode
    def calc_tendance_multi(row):
        d1 = row.get('diff_p1', 0) or 0
        d2 = row.get('diff_p2', 0) or 0
        
        if pd.isna(d1): d1 = 0
        if pd.isna(d2): d2 = 0
        
        if d1 < -5 and d2 < -5:
            return "üìâüìâ Chute continue"
        elif d1 > 5 and d2 < -5:
            return "üìàüìâ Rebond puis rechute"
        elif d1 < -5 and d2 > 5:
            return "üìâüìà R√©cup√©ration"
        elif d1 > 5 and d2 > 5:
            return "üìàüìà Hausse continue"
        elif abs(d1) <= 5 and abs(d2) <= 5:
            return "‚û°Ô∏è Stable"
        elif d1 < 0 or d2 < 0:
            return "üìâ Baisse"
        else:
            return "üìà Hausse"
    
    df['tendance_multi'] = df.apply(calc_tendance_multi, axis=1)
    
    # Recalculer le volume si n√©cessaire
    if 'volume' not in df.columns and 'volumeh' in df.columns:
        df['volume'] = df['volumeh']
    
    # Recalculer priority_score
    if 'volume' in df.columns:
        df['priority_score'] = df['volume'].fillna(0) * df['diff_pos'].abs().fillna(0)
    else:
        df['priority_score'] = df['diff_pos'].abs().fillna(0)
    
    st.sidebar.info(f"üîó {len(df):,} KW fusionn√©s (P1: {len(df_p1):,} | P2: {len(df_p2):,})")

elif uploaded_file:
    df = load_data(uploaded_file)
    has_dual_haloscan = False

# Suite du traitement si on a des donn√©es
if (has_dual_haloscan or uploaded_file) and 'df' in dir():
    
    # Croiser avec les donn√©es leads si disponibles
    if has_leads and 'url' in df.columns:
        df['url_normalized'] = df['url'].apply(normalize_url)
        df = df.merge(
            leads_df[['url_normalized', 'leads_total', 'leads_avant', 'leads_apres', 'leads_evolution', 'leads_evolution_pct']], 
            on='url_normalized', 
            how='left'
        )
        
        # Stocker les labels de p√©riode pour l'affichage
        df.attrs['periode_avant_label'] = periode_avant_label
        df.attrs['periode_apres_label'] = periode_apres_label
        
        # Cr√©er indicateur visuel de tendance leads
        def tendance_leads(row):
            evol = row.get('leads_evolution', 0) or 0
            pct = row.get('leads_evolution_pct', 0) or 0
            if evol < -10 or pct < -20:
                return "üîªüîª CHUTE"
            elif evol < 0:
                return "üîª Baisse"
            elif evol == 0:
                return "‚û°Ô∏è Stable"
            elif evol > 10 or pct > 20:
                return "üî∫üî∫ BOOM"
            else:
                return "üî∫ Hausse"
        
        df['tendance_leads'] = df.apply(tendance_leads, axis=1)
        
        # Score de priorit√© enrichi : 
        # - priority_score = volume recherche √ó |diff_pos|
        # - On booste si l'URL g√©n√®re des leads (leads_total)
        # - On booste ENCORE PLUS si les leads sont en baisse (leads_evolution < 0)
        base_score = df['priority_score']
        leads_boost = (1 + df['leads_total'].fillna(0) / 100)  # Plus de leads = plus important
        
        # Malus si les leads baissent (√©volution n√©gative)
        leads_trend = df['leads_evolution'].fillna(0)
        trend_multiplier = 1 + (leads_trend.clip(upper=0).abs() / 100)  # Perte de leads = urgence
        
        df['priority_score_business'] = base_score * leads_boost * trend_multiplier
        
        # Flag pour identifier les URLs en double peine (perte SEO + perte leads)
        df['double_peine'] = (df['diff_pos'] < 0) & (df['leads_evolution'] < 0)
        
        # Cr√©er indicateur visuel de tendance SEO (positions)
        def tendance_seo(diff):
            if pd.isna(diff):
                return "‚û°Ô∏è N/A"
            diff = int(diff)
            if diff <= -20:
                return "üîªüîª CHUTE"
            elif diff < 0:
                return "üîª Baisse"
            elif diff == 0:
                return "‚û°Ô∏è Stable"
            elif diff >= 20:
                return "üî∫üî∫ BOOM"
            else:
                return "üî∫ Hausse"
        
        df['tendance_seo'] = df['diff_pos'].apply(tendance_seo)
        
        st.success(f"‚úÖ {len(df):,} mots-cl√©s charg√©s ‚Äî Donn√©es leads crois√©es !")
        
        # Stats de matching
        urls_avec_leads = df[df['leads_total'].notna() & (df['leads_total'] > 0)]['url'].nunique()
        urls_double_peine = df[df['double_peine'] == True]['url'].nunique()
        st.info(f"üìä {urls_avec_leads} URLs avec leads | ‚ö†Ô∏è {urls_double_peine} URLs en double peine (perte SEO + perte leads)")
        
        has_leads_merged = True
    else:
        df['leads_total'] = 0
        df['leads_avant'] = 0
        df['leads_apres'] = 0
        df['leads_evolution'] = 0
        df['tendance_leads'] = "‚û°Ô∏è N/A"
        
        # Cr√©er indicateur visuel de tendance SEO (positions) m√™me sans leads
        def tendance_seo(diff):
            if pd.isna(diff):
                return "‚û°Ô∏è N/A"
            diff = int(diff)
            if diff <= -20:
                return "üîªüîª CHUTE"
            elif diff < 0:
                return "üîª Baisse"
            elif diff == 0:
                return "‚û°Ô∏è Stable"
            elif diff >= 20:
                return "üî∫üî∫ BOOM"
            else:
                return "üî∫ Hausse"
        
        df['tendance_seo'] = df['diff_pos'].apply(tendance_seo)
        
        if has_leads:
            st.warning("‚ö†Ô∏è Fichier leads charg√© mais colonne 'url' manquante dans le CSV Haloscan")
        st.success(f"‚úÖ {len(df):,} mots-cl√©s charg√©s")
        has_leads_merged = False
    
    # Debug colonnes
    with st.sidebar:
        with st.expander("üîç Colonnes", expanded=True):
            st.write(list(df.columns))
    
    # V√©rification diff_pos
    if 'diff_pos' not in df.columns:
        st.error(f"‚ùå Colonne 'diff_pos' non trouv√©e. Colonnes: {list(df.columns)}")
        st.stop()
    
    # ==========================================================================
    # FILTRES
    # ==========================================================================
    
    with st.sidebar:
        st.header("üéõÔ∏è Filtres")
        
        variation = st.multiselect("Variation", ['Pertes', 'Gains', 'Stables'], default=['Pertes', 'Gains', 'Stables'])
        
        if 'volume' in df.columns:
            vmin, vmax = int(df['volume'].min() or 0), int(df['volume'].max() or 10000)
            vol_range = st.slider("Volume", vmin, vmax, (vmin, vmax))
        else:
            vol_range = None
        
        search_kw = st.text_input("üîé Mot-cl√©")
        search_url = st.text_input("üîé URL contient")
    
    # Appliquer filtres
    df_f = df.copy()
    
    # Filtre variation
    masks = []
    if 'Pertes' in variation:
        masks.append(df_f['diff_pos'] < 0)
    if 'Gains' in variation:
        masks.append(df_f['diff_pos'] > 0)
    if 'Stables' in variation:
        masks.append(df_f['diff_pos'] == 0)
    if masks:
        combined = masks[0]
        for m in masks[1:]:
            combined = combined | m
        df_f = df_f[combined]
    
    # Filtre volume
    if vol_range and 'volume' in df_f.columns:
        df_f = df_f[(df_f['volume'] >= vol_range[0]) & (df_f['volume'] <= vol_range[1])]
    
    # Filtre recherche
    if search_kw and 'mot_cle' in df_f.columns:
        df_f = df_f[df_f['mot_cle'].astype(str).str.contains(search_kw, case=False, na=False)]
    if search_url and 'url' in df_f.columns:
        df_f = df_f[df_f['url'].astype(str).str.contains(search_url, case=False, na=False)]
    
    # ==========================================================================
    # KPIs
    # ==========================================================================
    
    total = len(df_f)
    pertes = len(df_f[df_f['diff_pos'] < 0])
    gains = len(df_f[df_f['diff_pos'] > 0])
    stables = len(df_f[df_f['diff_pos'] == 0])
    
    vol_perdu = int(df_f[df_f['diff_pos'] < 0]['volume'].fillna(0).sum()) if 'volume' in df_f.columns else 0
    vol_gagne = int(df_f[df_f['diff_pos'] > 0]['volume'].fillna(0).sum()) if 'volume' in df_f.columns else 0
    
    # ==========================================================================
    # ONGLETS
    # ==========================================================================
    
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["üìä Dashboard", "üî¥ Pertes", "üìÅ Par URL", "üü¢ Gains", "üîÑ Cannibalisation", "üìù Rapport"])
    
    # TAB 1: DASHBOARD
    with tab1:
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Total", f"{total:,}")
        c2.metric("üî¥ Pertes", f"{pertes:,}")
        c3.metric("üü¢ Gains", f"{gains:,}")
        c4.metric("‚ö™ Stables", f"{stables:,}")
        
        st.divider()
        
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("üìâ Volume perdu", f"{vol_perdu:,}")
        c2.metric("üìà Volume gagn√©", f"{vol_gagne:,}")
        
        # M√©triques leads si disponibles
        if has_leads_merged:
            # Leads sur les URLs en perte - NE PAS compter plusieurs fois la m√™me URL
            df_pertes_dash = df_f[df_f['diff_pos'] < 0]
            df_urls_perte_unique = df_pertes_dash.drop_duplicates(subset=['url']) if 'url' in df_pertes_dash.columns else df_pertes_dash
            
            leads_urls_perte = df_urls_perte_unique['leads_total'].fillna(0).sum()
            c3.metric("‚ö†Ô∏è Leads sur URLs en perte", f"{int(leads_urls_perte):,}")
            
            leads_evol = df_urls_perte_unique['leads_evolution'].fillna(0).sum()
            delta_color = "inverse" if leads_evol < 0 else "normal"
            c4.metric("üìä √âvol. leads (p√©riode)", f"{int(leads_evol):+,}", delta_color=delta_color)
        
        # Section MULTI-P√âRIODES si disponible
        if has_dual_haloscan and 'tendance_multi' in df_f.columns:
            st.divider()
            st.subheader(f"üìà Analyse multi-p√©riodes ({label_debut_p1} ‚Üí {label_fin_p1} ‚Üí {label_fin_p2})")
            
            # Compter les tendances
            tendances = df_f['tendance_multi'].value_counts()
            
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("üìâüìâ Chute continue", f"{tendances.get('üìâüìâ Chute continue', 0):,}", help="Perte P1 ET perte P2")
            col2.metric("üìàüìâ Rebond puis rechute", f"{tendances.get('üìàüìâ Rebond puis rechute', 0):,}", help="Gain P1 puis perte P2")
            col3.metric("üìâüìà R√©cup√©ration", f"{tendances.get('üìâüìà R√©cup√©ration', 0):,}", help="Perte P1 puis gain P2")
            col4.metric("üìàüìà Hausse continue", f"{tendances.get('üìàüìà Hausse continue', 0):,}", help="Gain P1 ET gain P2")
            
            # Tableau des KW en chute continue (priorit√© max)
            df_chute_continue = df_f[df_f['tendance_multi'] == 'üìâüìâ Chute continue'].copy()
            if len(df_chute_continue) > 0:
                st.error(f"üö® **{len(df_chute_continue):,}** mots-cl√©s en CHUTE CONTINUE ‚Äî Probl√®me structurel √† traiter !")
                
                # Afficher les colonnes pertinentes
                cols_multi = ['mot_cle', 'url', 'pos_debut_p1', 'pos_fin_p1', 'diff_p1', 'pos_fin_p2', 'diff_p2', 'diff_pos', 'volume']
                cols_multi = [c for c in cols_multi if c in df_chute_continue.columns]
                
                # Renommer pour clart√© avec labels dynamiques
                df_chute_display = df_chute_continue[cols_multi].head(50).copy()
                rename_map = {
                    'pos_debut_p1': f'Pos {label_debut_p1}',
                    'pos_fin_p1': f'Pos {label_fin_p1}',
                    'diff_p1': f'Œî P1',
                    'pos_fin_p2': f'Pos {label_fin_p2}',
                    'diff_p2': f'Œî P2',
                    'diff_pos': 'Œî TOTAL',
                    'volume': 'Volume'
                }
                df_chute_display = df_chute_display.rename(columns=rename_map)
                
                st.dataframe(df_chute_display.sort_values('Œî TOTAL', ascending=True), use_container_width=True, height=300)
        
        # Section DOUBLE PEINE (suite du code existant)
        if has_leads_merged:
            if 'double_peine' in df_f.columns:
                df_double_peine = df_f[df_f['double_peine'] == True]
                if len(df_double_peine) > 0:
                    st.divider()
                    st.subheader("üö® ALERTE : URLs en DOUBLE PEINE (perte SEO + perte leads)")
                    st.error(f"**{df_double_peine['url'].nunique()}** URLs perdent √† la fois des positions ET des leads !")
                    
                    # R√©cup√©rer les labels de p√©riode
                    p_avant = df.attrs.get('periode_avant_label', 'AVANT')
                    p_apres = df.attrs.get('periode_apres_label', 'APR√àS')
                    
                    # Tableau des URLs double peine
                    agg_dp = {'diff_pos': ['count', 'sum']}
                    if 'tendance_seo' in df_double_peine.columns:
                        agg_dp['tendance_seo'] = lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else "‚û°Ô∏è N/A"
                    if 'leads_avant' in df_double_peine.columns:
                        agg_dp['leads_avant'] = 'first'
                    if 'leads_apres' in df_double_peine.columns:
                        agg_dp['leads_apres'] = 'first'
                    if 'leads_evolution' in df_double_peine.columns:
                        agg_dp['leads_evolution'] = 'first'
                    if 'tendance_leads' in df_double_peine.columns:
                        agg_dp['tendance_leads'] = 'first'
                    
                    df_dp_urls = df_double_peine.groupby('url').agg(agg_dp).reset_index()
                    df_dp_urls.columns = ['URL', 'KW perdus', 'Diff total'] + \
                                        (['üìä SEO'] if 'tendance_seo' in df_double_peine.columns else []) + \
                                        ([f'Leads {p_avant}'] if 'leads_avant' in df_double_peine.columns else []) + \
                                        ([f'Leads {p_apres}'] if 'leads_apres' in df_double_peine.columns else []) + \
                                        (['√âvol. Leads'] if 'leads_evolution' in df_double_peine.columns else []) + \
                                        (['üìä LEADS'] if 'tendance_leads' in df_double_peine.columns else [])
                    
                    # Trier par √©volution leads (les plus grosses pertes en premier)
                    if '√âvol. Leads' in df_dp_urls.columns:
                        df_dp_urls = df_dp_urls.sort_values('√âvol. Leads', ascending=True)
                    elif 'Diff total' in df_dp_urls.columns:
                        df_dp_urls = df_dp_urls.sort_values('Diff total', ascending=True)
                    
                    st.dataframe(df_dp_urls.head(20), use_container_width=True, hide_index=True)
        
        st.divider()
        
        col1, col2 = st.columns(2)
        with col1:
            fig = px.pie(values=[pertes, gains, stables], names=['Pertes', 'Gains', 'Stables'],
                        color_discrete_sequence=['#EF4444', '#22C55E', '#6B7280'])
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = px.histogram(df_f, x='diff_pos', nbins=50)
            st.plotly_chart(fig, use_container_width=True)
        
        # Top URLs impact√©es avec leads
        try:
            df_pertes_temp = df_f[df_f['diff_pos'] < 0]
            if has_leads_merged and len(df_pertes_temp) > 0 and 'leads_total' in df_pertes_temp.columns:
                st.subheader("üéØ URLs critiques : Pertes SEO + Impact Business")
                
                # Construire l'agr√©gation dynamiquement selon les colonnes disponibles
                agg_dict_dash = {'diff_pos': ('diff_pos', 'count')}
                if 'volume' in df_pertes_temp.columns:
                    agg_dict_dash['volume_perdu'] = ('volume', 'sum')
                if 'leads_total' in df_pertes_temp.columns:
                    agg_dict_dash['leads_total'] = ('leads_total', 'first')
                if 'leads_evolution' in df_pertes_temp.columns:
                    agg_dict_dash['leads_evolution'] = ('leads_evolution', 'first')
                if 'tendance_leads' in df_pertes_temp.columns:
                    agg_dict_dash['tendance_leads'] = ('tendance_leads', 'first')
                
                df_perte_urls = df_pertes_temp.groupby('url').agg(**agg_dict_dash).reset_index()
                df_perte_urls = df_perte_urls.rename(columns={'diff_pos': 'kw_perdus'})
                
                # Trier par √©volution leads (les plus grosses pertes en premier)
                if 'leads_evolution' in df_perte_urls.columns:
                    df_perte_urls = df_perte_urls.sort_values('leads_evolution', ascending=True).head(15)
                else:
                    df_perte_urls = df_perte_urls.sort_values('kw_perdus', ascending=False).head(15)
                
                st.dataframe(df_perte_urls, use_container_width=True)
        except Exception as e:
            st.warning(f"Impossible d'afficher les URLs critiques: {e}")
    
    # TAB 2: PERTES
    with tab2:
        st.header("üî¥ Pertes critiques")
        df_pertes = df_f[df_f['diff_pos'] < 0].sort_values('diff_pos', ascending=True)
        st.info(f"**{len(df_pertes):,}** mots-cl√©s en perte")
        
        cols = [c for c in ['mot_cle', 'url', 'ancienne_pos', 'derniere_pos', 'diff_pos', 'tendance_seo', 'volume'] if c in df_pertes.columns]
        st.dataframe(df_pertes[cols], use_container_width=True, height=600)
        
        csv = df_pertes[cols].to_csv(index=False, sep=';').encode('utf-8')
        st.download_button("üì• Export CSV", csv, "pertes.csv")
    
    # TAB 3: PAR URL
    with tab3:
        st.header("üìÅ Analyse par URL")
        if 'url' in df_f.columns:
            try:
                # Construire l'agr√©gation dynamiquement
                agg_funcs = {
                    'diff_pos': ['count', lambda x: (x < 0).sum(), lambda x: (x > 0).sum(), 'sum'],
                }
                if 'volume' in df_f.columns:
                    agg_funcs['volume'] = 'sum'
                if has_leads_merged:
                    if 'leads_total' in df_f.columns:
                        agg_funcs['leads_total'] = 'first'
                    if 'leads_avant' in df_f.columns:
                        agg_funcs['leads_avant'] = 'first'
                    if 'leads_apres' in df_f.columns:
                        agg_funcs['leads_apres'] = 'first'
                    if 'leads_evolution' in df_f.columns:
                        agg_funcs['leads_evolution'] = 'first'
                    if 'tendance_leads' in df_f.columns:
                        agg_funcs['tendance_leads'] = 'first'
                
                url_stats = df_f.groupby('url').agg(agg_funcs).reset_index()
                
                # Aplatir les colonnes multi-index
                new_cols = ['url']
                for col in url_stats.columns[1:]:
                    if isinstance(col, tuple):
                        new_cols.append(f"{col[0]}_{col[1]}" if col[1] != 'sum' and col[1] != 'first' else col[0])
                    else:
                        new_cols.append(col)
                url_stats.columns = new_cols
                
                # Renommer les colonnes pour plus de clart√©
                rename_dict = {
                    'diff_pos_count': 'total_kw',
                    'diff_pos_<lambda_0>': 'kw_perte', 
                    'diff_pos_<lambda_1>': 'kw_gain',
                    'diff_pos_sum': 'diff_total'
                }
                url_stats = url_stats.rename(columns=rename_dict)
                
                # Ajouter indicateur tendance SEO bas√© sur diff_total
                def tendance_seo_url(diff):
                    if pd.isna(diff):
                        return "‚û°Ô∏è N/A"
                    diff = int(diff)
                    if diff <= -50:
                        return "üîªüîª CHUTE"
                    elif diff < 0:
                        return "üîª Baisse"
                    elif diff == 0:
                        return "‚û°Ô∏è Stable"
                    elif diff >= 50:
                        return "üî∫üî∫ BOOM"
                    else:
                        return "üî∫ Hausse"
                
                if 'diff_total' in url_stats.columns:
                    url_stats['üìä SEO'] = url_stats['diff_total'].apply(tendance_seo_url)
                
                # Ajouter tendance leads si dispo
                if 'tendance_leads' in url_stats.columns:
                    url_stats = url_stats.rename(columns={'tendance_leads': 'üìä LEADS'})
                
                # Tri par √©volution leads ou par nombre de KW en perte
                if 'leads_evolution' in url_stats.columns:
                    url_stats = url_stats.sort_values('leads_evolution', ascending=True)
                elif 'kw_perte' in url_stats.columns:
                    url_stats = url_stats.sort_values('kw_perte', ascending=False)
                else:
                    url_stats = url_stats.sort_values('total_kw', ascending=False)
                
                st.info(f"**{len(url_stats):,}** URLs analys√©es")
                st.dataframe(url_stats, use_container_width=True, height=500)
                
                # Export
                csv_urls = url_stats.to_csv(index=False, sep=';').encode('utf-8')
                st.download_button("üì• Exporter TOUTES les URLs (CSV)", csv_urls, "analyse_urls_complete.csv")
                
            except Exception as e:
                st.error(f"Erreur lors de l'analyse par URL: {e}")
            
            st.divider()
            
            # D√©tail URL
            st.subheader("üîç D√©tail d'une URL")
            url_list = df_f['url'].unique().tolist()[:100]
            url_sel = st.selectbox("S√©lectionner une URL", url_list)
            if url_sel:
                df_url = df_f[df_f['url'] == url_sel]
                
                c1, c2, c3, c4 = st.columns(4)
                c1.metric("Total KW", len(df_url))
                c2.metric("En perte", len(df_url[df_url['diff_pos'] < 0]))
                c3.metric("En gain", len(df_url[df_url['diff_pos'] > 0]))
                if 'volume' in df_url.columns:
                    c4.metric("Volume total", f"{int(df_url['volume'].fillna(0).sum()):,}")
                
                # Afficher les leads si dispo
                if has_leads_merged and 'leads_total' in df_url.columns:
                    # R√©cup√©rer les labels de p√©riode
                    p_avant = df.attrs.get('periode_avant_label', 'AVANT')
                    p_apres = df.attrs.get('periode_apres_label', 'APR√àS')
                    
                    c1, c2, c3, c4 = st.columns(4)
                    leads_t = df_url['leads_total'].iloc[0] if len(df_url) > 0 else 0
                    leads_av = df_url['leads_avant'].iloc[0] if len(df_url) > 0 and 'leads_avant' in df_url.columns else 0
                    leads_ap = df_url['leads_apres'].iloc[0] if len(df_url) > 0 and 'leads_apres' in df_url.columns else 0
                    leads_e = df_url['leads_evolution'].iloc[0] if len(df_url) > 0 and 'leads_evolution' in df_url.columns else 0
                    c1.metric("üìä Leads total", f"{int(leads_t or 0):,}")
                    c2.metric(f"üìä Leads {p_avant}", f"{int(leads_av or 0):,}")
                    c3.metric(f"üìä Leads {p_apres}", f"{int(leads_ap or 0):,}")
                    c4.metric("üìà √âvolution", f"{int(leads_e or 0):+,}")
                
                cols = [c for c in ['mot_cle', 'diff_pos', 'volume', 'derniere_pos', 'ancienne_pos', 'meilleure_pos'] if c in df_url.columns]
                st.dataframe(df_url[cols].sort_values('diff_pos'), use_container_width=True)
                
                # Export d√©tail URL
                csv_url_detail = df_url[cols].to_csv(index=False, sep=';').encode('utf-8')
                st.download_button(f"üì• Exporter les KW de cette URL", csv_url_detail, f"detail_url.csv")
        else:
            st.warning("Colonne 'url' non trouv√©e")
    
    # TAB 4: GAINS
    with tab4:
        st.header("üü¢ Gains")
        df_gains = df_f[df_f['diff_pos'] > 0].sort_values('diff_pos', ascending=False)
        st.success(f"**{len(df_gains):,}** mots-cl√©s en gain")
        
        cols = [c for c in ['mot_cle', 'url', 'diff_pos', 'tendance_seo', 'volume', 'derniere_pos', 'ancienne_pos'] if c in df_gains.columns]
        st.dataframe(df_gains[cols], use_container_width=True, height=600)
        
        csv_gains = df_gains[cols].to_csv(index=False, sep=';').encode('utf-8')
        st.download_button("üì• Exporter TOUS les gains (CSV)", csv_gains, "gains_complet.csv")
    
    # TAB 5: CANNIBALISATION
    with tab5:
        st.header("üîÑ D√©tection de cannibalisation interne")
        st.info("**Objectif** : Identifier les KW o√π une URL perd des positions tandis qu'une autre URL du site en gagne. Avant de r√©optimiser une page en perte, v√©rifiez qu'une autre page n'a pas pris le relais !")
        
        if 'mot_cle' in df.columns and 'url' in df.columns:
            with st.spinner("Analyse des cannibalisations en cours..."):
                # Travailler sur le df complet (pas filtr√©) pour d√©tecter toutes les cannibalisations
                df_canni = df[['mot_cle', 'url', 'ancienne_pos', 'derniere_pos', 'diff_pos', 'volume']].copy()
                
                # Pour chaque KW, trouver les URLs en perte et en gain
                df_pertes_canni = df_canni[df_canni['diff_pos'] < 0].copy()
                df_gains_canni = df_canni[df_canni['diff_pos'] > 0].copy()
                
                # Trouver les KW qui ont √† la fois des pertes ET des gains (= cannibalisation potentielle)
                kw_en_perte = set(df_pertes_canni['mot_cle'].unique())
                kw_en_gain = set(df_gains_canni['mot_cle'].unique())
                kw_cannibalisation = kw_en_perte & kw_en_gain
                
                st.metric("üîÑ KW avec cannibalisation potentielle", f"{len(kw_cannibalisation):,}")
                
                if len(kw_cannibalisation) > 0:
                    # Construire le tableau de cannibalisation
                    resultats_canni = []
                    
                    for kw in kw_cannibalisation:
                        # URLs en perte sur ce KW
                        urls_perte = df_pertes_canni[df_pertes_canni['mot_cle'] == kw].sort_values('diff_pos', ascending=True)
                        # URLs en gain sur ce KW
                        urls_gain = df_gains_canni[df_gains_canni['mot_cle'] == kw].sort_values('diff_pos', ascending=False)
                        
                        # Prendre la pire perte et le meilleur gain
                        if len(urls_perte) > 0 and len(urls_gain) > 0:
                            perte = urls_perte.iloc[0]
                            gain = urls_gain.iloc[0]
                            
                            # Volume du KW (prendre le max disponible)
                            vol = max(perte.get('volume', 0) or 0, gain.get('volume', 0) or 0)
                            
                            resultats_canni.append({
                                'mot_cle': kw,
                                'volume': vol,
                                'url_perte': perte['url'],
                                'ancienne_pos_perte': perte.get('ancienne_pos', 0),
                                'nouvelle_pos_perte': perte.get('derniere_pos', 0),
                                'diff_perte': perte.get('diff_pos', 0),
                                'url_gain': gain['url'],
                                'ancienne_pos_gain': gain.get('ancienne_pos', 0),
                                'nouvelle_pos_gain': gain.get('derniere_pos', 0),
                                'diff_gain': gain.get('diff_pos', 0),
                            })
                    
                    if resultats_canni:
                        df_resultats = pd.DataFrame(resultats_canni)
                        
                        # Trier par volume d√©croissant (les KW les plus importants d'abord)
                        df_resultats = df_resultats.sort_values('volume', ascending=False)
                        
                        # Filtres
                        col1, col2 = st.columns(2)
                        with col1:
                            vol_min_canni = st.number_input("Volume minimum", min_value=0, value=0, step=50, key="vol_canni")
                        with col2:
                            diff_min_canni = st.number_input("Perte minimum (positions)", min_value=0, value=0, step=1, key="diff_canni")
                        
                        # Appliquer filtres (fillna pour √©viter que les NaN soient exclus)
                        df_resultats_f = df_resultats[
                            (df_resultats['volume'].fillna(0) >= vol_min_canni) & 
                            (df_resultats['diff_perte'].fillna(0).abs() >= diff_min_canni)
                        ]
                        
                        st.success(f"**{len(df_resultats_f):,}** cas de cannibalisation d√©tect√©s (sur {len(df_resultats):,} total)")
                        
                        # Affichage du tableau
                        st.subheader("‚ö†Ô∏è KW √† risque ‚Äî V√©rifier avant r√©optimisation")
                        
                        # Formater pour l'affichage
                        df_display = df_resultats_f.copy()
                        df_display['üìâ URL en perte'] = df_display['url_perte']
                        df_display['√âtait pos'] = df_display['ancienne_pos_perte'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['‚Üí Maintenant'] = df_display['nouvelle_pos_perte'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['Diff'] = df_display['diff_perte'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['üìà URL en hausse'] = df_display['url_gain']
                        df_display['√âtait pos '] = df_display['ancienne_pos_gain'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['‚Üí Maintenant '] = df_display['nouvelle_pos_gain'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['Diff '] = df_display['diff_gain'].apply(lambda x: f"+{int(x)}" if pd.notna(x) else "+0")
                        df_display['Volume'] = df_display['volume'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        
                        cols_display = ['mot_cle', 'Volume', 'üìâ URL en perte', '√âtait pos', '‚Üí Maintenant', 'Diff', 'üìà URL en hausse', '√âtait pos ', '‚Üí Maintenant ', 'Diff ']
                        
                        st.dataframe(df_display[cols_display].head(100), use_container_width=True, height=500)
                        
                        # Alerte
                        st.warning("""
                        **‚ö†Ô∏è ATTENTION avant de r√©optimiser une URL en perte :**
                        1. V√©rifiez si l'URL en hausse r√©pond mieux √† l'intention de recherche
                        2. Si oui ‚Üí renforcez l'URL en hausse plut√¥t que l'ancienne
                        3. Si non ‚Üí v√©rifiez le maillage interne pour √©viter la cannibalisation
                        4. Envisagez une redirection 301 si l'ancienne URL n'a plus de raison d'√™tre
                        """)
                        
                        # Export
                        csv_canni = df_resultats_f.to_csv(index=False, sep=';').encode('utf-8')
                        st.download_button("üì• Exporter les cannibalisations (CSV)", csv_canni, "cannibalisations.csv")
                        
                else:
                    st.success("‚úÖ Aucune cannibalisation d√©tect√©e ! Chaque KW n'a qu'une seule URL qui bouge.")
        else:
            st.warning("Colonnes 'mot_cle' et 'url' n√©cessaires pour l'analyse de cannibalisation")
    
    # TAB 6: RAPPORT
    with tab6:
        st.header("üìù Rapport complet pour l'√©quipe √©dito")
        
        if st.button("üîÑ G√©n√©rer le rapport complet", type="primary"):
            
            # Calculs pour le rapport
            df_pertes_rapport = df_f[df_f['diff_pos'] < 0].sort_values('diff_pos', ascending=True)
            df_gains_rapport = df_f[df_f['diff_pos'] > 0].sort_values('diff_pos', ascending=False)
            
            # URLs les plus impact√©es
            urls_critiques = pd.DataFrame()  # Initialiser vide par d√©faut
            if 'url' in df_f.columns and len(df_pertes_rapport) > 0:
                # Construire l'agr√©gation dynamiquement selon les colonnes disponibles
                agg_url = {'diff_pos': 'count'}
                if 'volume' in df_pertes_rapport.columns:
                    agg_url['volume'] = 'sum'
                if has_leads_merged:
                    if 'leads_total' in df_pertes_rapport.columns:
                        agg_url['leads_total'] = 'first'
                    if 'leads_avant' in df_pertes_rapport.columns:
                        agg_url['leads_avant'] = 'first'
                    if 'leads_apres' in df_pertes_rapport.columns:
                        agg_url['leads_apres'] = 'first'
                    if 'leads_evolution' in df_pertes_rapport.columns:
                        agg_url['leads_evolution'] = 'first'
                    if 'tendance_leads' in df_pertes_rapport.columns:
                        agg_url['tendance_leads'] = 'first'
                
                try:
                    urls_critiques = df_pertes_rapport.groupby('url').agg(agg_url).reset_index()
                    
                    # Renommer les colonnes
                    rename_cols = {'diff_pos': 'nb_kw_perdus', 'volume': 'volume_impacte'}
                    urls_critiques = urls_critiques.rename(columns=rename_cols)
                    
                    # Trier par √©volution leads ou par nb KW perdus
                    if 'leads_evolution' in urls_critiques.columns:
                        urls_critiques = urls_critiques.sort_values('leads_evolution', ascending=True)
                    else:
                        urls_critiques = urls_critiques.sort_values('nb_kw_perdus', ascending=False)
                except Exception as e:
                    st.warning(f"Erreur agr√©gation URLs: {e}")
                    urls_critiques = pd.DataFrame()
            
            # Calcul impact leads - ATTENTION : √©viter de compter plusieurs fois la m√™me URL
            if has_leads_merged:
                # Grouper par URL pour ne compter qu'une fois les leads de chaque URL
                urls_en_perte = df_pertes_rapport['url'].unique() if 'url' in df_pertes_rapport.columns else []
                df_urls_perte_unique = df_pertes_rapport.drop_duplicates(subset=['url'])
                
                total_leads_perte = int(df_urls_perte_unique['leads_total'].fillna(0).sum())
                total_leads_avant_perte = int(df_urls_perte_unique['leads_avant'].fillna(0).sum())
                total_leads_apres_perte = int(df_urls_perte_unique['leads_apres'].fillna(0).sum())
                leads_evolution_total = int(df_urls_perte_unique['leads_evolution'].fillna(0).sum())
            
            # D√©finir la p√©riode pour le titre du rapport
            if has_dual_haloscan:
                periode_rapport = f"{label_debut_p1} ‚Üí {label_fin_p2}"
            else:
                periode_rapport = "P√©riode analys√©e"
            
            report = f"""# üìä RAPPORT D'ANALYSE SEO COMPLET
## P√©riode : {periode_rapport}
## G√©n√©r√© le {datetime.now().strftime('%d/%m/%Y √† %H:%M')}

---

# 1. SYNTH√àSE GLOBALE

| M√©trique | Valeur |
|----------|--------|
| **Total mots-cl√©s analys√©s** | {total:,} |
| **Mots-cl√©s en perte** | {pertes:,} ({pertes/total*100:.1f}%) |
| **Mots-cl√©s en gain** | {gains:,} ({gains/total*100:.1f}%) |
| **Mots-cl√©s stables** | {stables:,} ({stables/total*100:.1f}%) |
| **Volume de recherche perdu** | {vol_perdu:,} /mois |
| **Volume de recherche gagn√©** | {vol_gagne:,} /mois |
| **Bilan net volume** | {vol_gagne - vol_perdu:+,} /mois |
"""
            
            if has_leads_merged:
                periodes_info = f"P√©riode AVANT: {', '.join(periode_avant) if periode_avant else 'N/A'} | P√©riode APR√àS: {', '.join(periode_apres) if periode_apres else 'N/A'}"
                report += f"""
## üí∞ IMPACT BUSINESS (Leads)

**{periodes_info}**

| M√©trique | Valeur |
|----------|--------|
| **Leads historiques sur URLs en perte** | {total_leads_perte:,} |
| **Leads p√©riode AVANT** | {total_leads_avant_perte:,} |
| **Leads p√©riode APR√àS** | {total_leads_apres_perte:,} |
| **√âvolution des leads** | {leads_evolution_total:+,} |

‚ö†Ô∏è **Ces URLs g√©n√®rent des leads et perdent en visibilit√© SEO = PRIORIT√â MAXIMALE**

"""

            # Section multi-p√©riodes si disponible
            if has_dual_haloscan and 'tendance_multi' in df_f.columns:
                tendances = df_f['tendance_multi'].value_counts()
                chute_continue = tendances.get('üìâüìâ Chute continue', 0)
                rebond_rechute = tendances.get('üìàüìâ Rebond puis rechute', 0)
                recuperation = tendances.get('üìâüìà R√©cup√©ration', 0)
                hausse_continue = tendances.get('üìàüìà Hausse continue', 0)
                
                report += f"""---

## üìà ANALYSE MULTI-P√âRIODES ({label_debut_p1} ‚Üí {label_fin_p1} ‚Üí {label_fin_p2})

| Tendance | Nombre de KW | Signification |
|----------|--------------|---------------|
| üìâüìâ **Chute continue** | {chute_continue:,} | Perte sur P1 ET P2 ‚Äî **Probl√®me structurel** |
| üìàüìâ Rebond puis rechute | {rebond_rechute:,} | Gain sur P1 puis perte sur P2 |
| üìâüìà R√©cup√©ration | {recuperation:,} | Perte sur P1 puis gain sur P2 |
| üìàüìà Hausse continue | {hausse_continue:,} | Gain sur P1 ET P2 |

"""
                # Ajouter les KW en chute continue (TOP 100)
                df_chute_continue = df_f[df_f['tendance_multi'] == 'üìâüìâ Chute continue'].copy()
                if len(df_chute_continue) > 0:
                    report += f"""### üö® TOP 100 KW en CHUTE CONTINUE ‚Äî Priorit√© maximale

| Mot-cl√© | URL | Pos {label_debut_p1} | Pos {label_fin_p1} | Œî P1 | Pos {label_fin_p2} | Œî P2 | Œî TOTAL | Volume |
|---------|-----|---------------------|--------------------|----- |--------------------|----- |---------|--------|
"""
                    # Trier par diff totale
                    df_chute_continue = df_chute_continue.sort_values('diff_pos', ascending=True)
                    
                    for _, row in df_chute_continue.head(100).iterrows():
                        mc = str(row.get('mot_cle', 'N/A'))[:40]
                        url = str(row.get('url', 'N/A'))
                        pos_debut = int(row.get('pos_debut_p1', 0)) if pd.notna(row.get('pos_debut_p1')) else 0
                        pos_mid = int(row.get('pos_fin_p1', 0)) if pd.notna(row.get('pos_fin_p1')) else 0
                        diff_p1 = int(row.get('diff_p1', 0)) if pd.notna(row.get('diff_p1')) else 0
                        pos_fin = int(row.get('pos_fin_p2', 0)) if pd.notna(row.get('pos_fin_p2')) else 0
                        diff_p2 = int(row.get('diff_p2', 0)) if pd.notna(row.get('diff_p2')) else 0
                        diff_tot = int(row.get('diff_pos', 0)) if pd.notna(row.get('diff_pos')) else 0
                        vol = int(row.get('volume', 0)) if pd.notna(row.get('volume')) else 0
                        report += f"| {mc} | {url} | {pos_debut} | {pos_mid} | {diff_p1} | {pos_fin} | {diff_p2} | {diff_tot} | {vol:,} |\n"
                    
                    if len(df_chute_continue) > 100:
                        report += f"\n_+ {len(df_chute_continue) - 100:,} autres KW en chute continue (non affich√©s)_\n"

            report += """---

# 2. DIAGNOSTIC

"""
            if gains == 0:
                report += f"""‚ö†Ô∏è **SITUATION CRITIQUE** : Le site n'a aucun gain de position.
- {pertes:,} mots-cl√©s en perte
- Action recommand√©e : **Audit urgent des contenus**

"""
            elif pertes > gains:
                report += f"""‚ö†Ô∏è **SITUATION PR√âOCCUPANTE** : Le site perd plus de positions qu'il n'en gagne.
- Ratio pertes/gains : {pertes/gains:.1f}x plus de pertes
- Action recommand√©e : **Audit urgent des contenus impact√©s**

"""
            elif pertes == 0:
                report += f"""‚úÖ **SITUATION EXCELLENTE** : Aucune perte de position !
- {gains:,} mots-cl√©s en gain

"""
            else:
                report += f"""‚úÖ **SITUATION POSITIVE** : Le site gagne plus de positions qu'il n'en perd.
- Ratio gains/pertes : {gains/pertes:.1f}x plus de gains

"""

            if len(urls_critiques) > 0:
                report += f"""---

# 3. TOUTES LES PAGES √Ä TRAITER ({len(urls_critiques):,} URLs)

"""
                if has_leads_merged:
                    # R√©cup√©rer les labels de p√©riode
                    p_avant = df.attrs.get('periode_avant_label', 'AVANT')
                    p_apres = df.attrs.get('periode_apres_label', 'APR√àS')
                    
                    report += f"""**Tri√©es par √©volution des leads** ‚Äî Les URLs avec les plus grosses pertes de leads en premier.

| Priorit√© | URL | KW perdus | Volume | Leads {p_avant} | Leads {p_apres} | üìä TENDANCE |
|----------|-----|-----------|--------|-------------|-------------|-------------|
"""
                    for i, row in urls_critiques.iterrows():
                        leads_evol = row.get('leads_evolution', 0)
                        leads_evol = 0 if pd.isna(leads_evol) else leads_evol
                        tendance = row.get('tendance_leads', '‚û°Ô∏è N/A')
                        prio = "üî¥ CRITIQUE" if leads_evol < -100 else \
                               "üü† URGENT" if leads_evol < -20 else \
                               "üü° MOYEN" if leads_evol < 0 else "‚ö™ STABLE/HAUSSE"
                        
                        # S√©curiser toutes les valeurs num√©riques
                        nb_kw = int(row.get('nb_kw_perdus', 0) or 0)
                        vol = row.get('volume_impacte', 0)
                        vol = 0 if pd.isna(vol) else int(vol)
                        l_avant = row.get('leads_avant', 0)
                        l_avant = 0 if pd.isna(l_avant) else int(l_avant)
                        l_apres = row.get('leads_apres', 0)
                        l_apres = 0 if pd.isna(l_apres) else int(l_apres)
                        
                        report += f"| {prio} | {row['url']} | {nb_kw} | {vol:,} | {l_avant:,} | {l_apres:,} | {tendance} |\n"
                else:
                    report += """**Tri√©es par nombre de mots-cl√©s perdus**

| Priorit√© | URL | KW perdus | Volume impact√© |
|----------|-----|-----------|----------------|
"""
                    for i, row in urls_critiques.iterrows():
                        nb_kw = row['nb_kw_perdus']
                        prio = "üî¥ URGENT" if nb_kw > 50 else "üü† MOYEN" if nb_kw > 10 else "üü° FAIBLE"
                        report += f"| {prio} | {row['url']} | {int(nb_kw)} | {int(row.get('volume_impacte', 0) or 0):,} |\n"
            else:
                report += """---

# 3. PAGES √Ä TRAITER

_Aucune URL en perte d√©tect√©e_

"""

            # Filtrer les KW qui ont vraiment morfl√© (grosses pertes uniquement)
            # Priorit√© : diff tr√®s n√©gative + volume √©lev√©
            df_pertes_critiques = df_pertes_rapport[df_pertes_rapport['diff_pos'] <= -5].copy()
            
            # Grouper par URL et garder le KW principal :
            # = celui avec le plus gros volume PARMI ceux o√π l'URL rankait bien avant (ancienne_pos ‚â§ 10)
            if 'volume' in df_pertes_critiques.columns and 'url' in df_pertes_critiques.columns:
                # Filtrer les KW o√π l'URL rankait vraiment bien (top 10)
                df_bien_ranke = df_pertes_critiques[df_pertes_critiques['ancienne_pos'] <= 10].copy()
                
                # Si pas de KW bien rank√© pour une URL, on prend quand m√™me le meilleur volume
                if len(df_bien_ranke) > 0:
                    idx_kw_principal = df_bien_ranke.groupby('url')['volume'].idxmax()
                    df_pertes_par_url = df_bien_ranke.loc[idx_kw_principal].copy()
                else:
                    idx_kw_principal = df_pertes_critiques.groupby('url')['volume'].idxmax()
                    df_pertes_par_url = df_pertes_critiques.loc[idx_kw_principal].copy()
                
                # Ajouter le nombre total de KW perdus par URL (tous les KW, pas que les bien rank√©s)
                kw_count = df_pertes_critiques.groupby('url').size().rename('nb_kw_perdus')
                df_pertes_par_url = df_pertes_par_url.merge(kw_count, on='url', how='left')
                
                # Trier par diff_pos (les pires en premier)
                df_pertes_par_url = df_pertes_par_url.sort_values('diff_pos', ascending=True)
            else:
                df_pertes_par_url = df_pertes_critiques.sort_values('diff_pos', ascending=True)
            
            # Limiter √† 500 URLs max
            max_kw_rapport = 500
            df_pertes_limited = df_pertes_par_url.head(max_kw_rapport)
            
            report += f"""

---

# 4. PERTES CRITIQUES ‚Äî TOP {len(df_pertes_limited):,} URLs (pertes ‚â• 5 positions)

**‚ö†Ô∏è Priorit√© maximale ‚Äî KW principal = plus gros volume parmi ceux o√π l'URL √©tait en top 10**

| KW Principal | URL | Ancienne pos | Nouvelle pos | Diff | Volume | Nb KW perdus |
|--------------|-----|--------------|--------------|------|--------|--------------|
"""
            for _, row in df_pertes_limited.iterrows():
                mc = str(row.get('mot_cle', 'N/A'))[:50]
                url = str(row.get('url', 'N/A'))
                anc = row.get('ancienne_pos', 0)
                anc = 0 if pd.isna(anc) else int(anc)
                dern = row.get('derniere_pos', 0)
                dern = 0 if pd.isna(dern) else int(dern)
                diff = row.get('diff_pos', 0)
                diff = 0 if pd.isna(diff) else int(diff)
                vol = row.get('volume', 0)
                vol = 0 if pd.isna(vol) else int(vol)
                nb_kw = row.get('nb_kw_perdus', 1)
                nb_kw = 1 if pd.isna(nb_kw) else int(nb_kw)
                report += f"| {mc} | {url} | {anc} | {dern} | {diff} | {vol:,} | {nb_kw} |\n"
            
            # Info sur les URLs non affich√©es
            nb_autres_urls = len(df_pertes_par_url) - len(df_pertes_limited)
            if nb_autres_urls > 0:
                report += f"\n_+ {nb_autres_urls:,} autres URLs avec des pertes ‚â• 5 positions (non affich√©es)_\n"

            # Filtrer les KW avec gains significatifs (‚â• 5 positions)
            df_gains_significatifs = df_gains_rapport[df_gains_rapport['diff_pos'] >= 5].copy()
            
            # Grouper par URL et garder le KW principal :
            # = celui avec le plus gros volume PARMI ceux o√π l'URL ranke bien maintenant (derniere_pos ‚â§ 10)
            if 'volume' in df_gains_significatifs.columns and 'url' in df_gains_significatifs.columns and len(df_gains_significatifs) > 0:
                # Filtrer les KW o√π l'URL ranke vraiment bien maintenant (top 10)
                df_bien_ranke = df_gains_significatifs[df_gains_significatifs['derniere_pos'] <= 10].copy()
                
                if len(df_bien_ranke) > 0:
                    idx_kw_principal = df_bien_ranke.groupby('url')['volume'].idxmax()
                    df_gains_par_url = df_bien_ranke.loc[idx_kw_principal].copy()
                else:
                    idx_kw_principal = df_gains_significatifs.groupby('url')['volume'].idxmax()
                    df_gains_par_url = df_gains_significatifs.loc[idx_kw_principal].copy()
                
                kw_count = df_gains_significatifs.groupby('url').size().rename('nb_kw_gains')
                df_gains_par_url = df_gains_par_url.merge(kw_count, on='url', how='left')
                
                df_gains_par_url = df_gains_par_url.sort_values('diff_pos', ascending=False)
            else:
                df_gains_par_url = df_gains_significatifs.sort_values('diff_pos', ascending=False)
            
            df_gains_limited = df_gains_par_url.head(max_kw_rapport)
            
            report += f"""

---

# 5. GAINS SIGNIFICATIFS ‚Äî TOP {len(df_gains_limited):,} URLs (gains ‚â• 5 positions)

**‚úÖ Ce qui fonctionne ‚Äî KW principal = plus gros volume parmi ceux en top 10 actuel**

| KW Principal | URL | Ancienne pos | Nouvelle pos | Diff | Volume | Nb KW gagn√©s |
|--------------|-----|--------------|--------------|------|--------|--------------|
"""
            for _, row in df_gains_limited.iterrows():
                mc = str(row.get('mot_cle', 'N/A'))[:50]
                url = str(row.get('url', 'N/A'))
                anc = row.get('ancienne_pos', 0)
                anc = 0 if pd.isna(anc) else int(anc)
                dern = row.get('derniere_pos', 0)
                dern = 0 if pd.isna(dern) else int(dern)
                diff = row.get('diff_pos', 0)
                diff = 0 if pd.isna(diff) else int(diff)
                vol = row.get('volume', 0)
                vol = 0 if pd.isna(vol) else int(vol)
                nb_kw = row.get('nb_kw_gains', 1)
                nb_kw = 1 if pd.isna(nb_kw) else int(nb_kw)
                report += f"| {mc} | {url} | {anc} | {dern} | +{diff} | {vol:,} | {nb_kw} |\n"
            
            nb_autres_urls = len(df_gains_par_url) - len(df_gains_limited)
            if nb_autres_urls > 0:
                report += f"\n_+ {nb_autres_urls:,} autres URLs avec des gains ‚â• 5 positions (non affich√©es)_\n"

            report += f"""

---

# 6. RECOMMANDATIONS POUR L'√âQUIPE √âDITO

## üî¥ Actions imm√©diates (cette semaine)
"""
            if has_leads_merged:
                report += """1. **PRIORIT√â ABSOLUE : URLs avec leads + pertes SEO** ‚Äî Ces pages g√©n√®rent du business ET perdent en visibilit√©
2. **Auditer le contenu** des 10 premi√®res URLs critiques
3. **V√©rifier le maillage interne** vers ces pages strat√©giques
"""
            else:
                report += """1. **Auditer les 10 premi√®res URLs critiques** ‚Äî V√©rifier : contenu √† jour ? maillage interne ? balises optimis√©es ?
2. **Identifier les KW √† fort volume perdus** ‚Äî Filtrer les pertes avec volume > 1000
3. **V√©rifier la concurrence** ‚Äî Les concurrents ont-ils am√©lior√© leur contenu ?
"""

            report += """
## üü† Actions court terme (ce mois)
1. **Mettre √† jour les contenus des pages critiques** ‚Äî Enrichir, actualiser, ajouter des sections
2. **Renforcer le maillage interne** vers les pages en perte
3. **Cr√©er du contenu de support** pour les th√©matiques en baisse

## üü° Actions moyen terme (ce trimestre)
1. **Audit technique** ‚Äî V√©rifier Core Web Vitals des pages impact√©es
2. **Analyse des backlinks** ‚Äî Les pages ont-elles perdu des liens ?
3. **Strat√©gie de contenu** ‚Äî Planifier les mises √† jour r√©currentes

---

# 7. M√âTRIQUES DE SUIVI

Refaire cette analyse dans 1 mois pour mesurer :
- [ ] R√©duction du nombre de KW en perte
- [ ] R√©cup√©ration des positions sur les KW prioritaires
- [ ] Am√©lioration du volume de recherche capt√©
"""
            if has_leads_merged:
                report += """- [ ] Stabilisation ou hausse des leads sur les URLs retravaill√©es
"""

            report += f"""
---

_Rapport g√©n√©r√© automatiquement ‚Äî Haloscan SEO Diff Analyzer_
_Donn√©es : {len(df):,} mots-cl√©s analys√©s"""
            
            if has_leads_merged:
                report += f" | {len(leads_df):,} URLs avec donn√©es leads"
            
            report += "_\n"
            
            st.session_state['report'] = report
            st.success("‚úÖ Rapport g√©n√©r√© !")
        
        if 'report' in st.session_state:
            st.markdown(st.session_state['report'])
            
            st.divider()
            
            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    "üì• T√©l√©charger le rapport (Markdown)", 
                    st.session_state['report'], 
                    "rapport_seo_complet.md",
                    "text/markdown"
                )
            with col2:
                # Export aussi en CSV les donn√©es brutes
                df_export = df_f[df_f['diff_pos'] < 0].sort_values('diff_pos', ascending=True)
                cols_export = [c for c in ['mot_cle', 'url', 'ancienne_pos', 'derniere_pos', 'diff_pos', 'tendance_seo', 'volume', 'leads_total', 'leads_avant', 'leads_apres', 'leads_evolution', 'tendance_leads'] if c in df_export.columns]
                csv_export = df_export[cols_export].to_csv(index=False, sep=';').encode('utf-8')
                st.download_button(
                    "üì• T√©l√©charger les donn√©es (CSV)",
                    csv_export,
                    "pertes_completes.csv",
                    "text/csv"
                )

else:
    st.info("üëÜ Charge un fichier CSV pour commencer")
