"""
Haloscan SEO Diff Analyzer
Version corrigÃ©e pour le format exact du fichier Baptiste
Avec intÃ©gration des donnÃ©es de leads par URL
"""

import streamlit as st
import pandas as pd
import plotly.express as px
from datetime import datetime
import json

st.set_page_config(
    page_title="Haloscan SEO Diff Analyzer",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =============================================================================
# CHARGEMENT DES DONNÃ‰ES
# =============================================================================

@st.cache_data
def load_data(uploaded_file):
    """Charge le CSV avec le bon sÃ©parateur (virgule)"""
    
    # Toujours utiliser la virgule comme sÃ©parateur
    try:
        df = pd.read_csv(uploaded_file, sep=',', encoding='utf-8')
    except UnicodeDecodeError:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, sep=',', encoding='latin-1')
    
    # Nettoyage des noms de colonnes
    df.columns = (df.columns
        .str.strip()
        .str.lower()
        .str.replace(' ', '_', regex=False)
        .str.replace(';', '', regex=False)
        .str.replace('.', '', regex=False)
        .str.replace('Ã©', 'e', regex=False)
        .str.replace('Ã¨', 'e', regex=False)
    )
    
    # Mapping vers noms standards
    mapping = {
        'mot-cle_(mc)': 'mot_cle',
        'plus_vieille_pos': 'ancienne_pos',
    }
    df = df.rename(columns=mapping)
    
    # CrÃ©er colonne 'volume' Ã  partir de 'volumeh' si elle n'existe pas
    if 'volume' not in df.columns and 'volumeh' in df.columns:
        df['volume'] = df['volumeh']
    
    # Conversion numÃ©rique
    for col in ['derniere_pos', 'ancienne_pos', 'meilleure_pos', 'diff_pos', 'volume', 'volumeh', 'trafic', 'cpc']:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Calcul du score de prioritÃ©
    vol = df['volume'].fillna(0) if 'volume' in df.columns else 0
    diff = df['diff_pos'].fillna(0).abs() if 'diff_pos' in df.columns else 0
    df['priority_score'] = vol * diff
    
    return df


def normalize_url(url):
    """Normalise une URL pour la comparaison"""
    if pd.isna(url):
        return ""
    url = str(url).lower().strip()
    # Retirer le protocole
    url = url.replace('https://', '').replace('http://', '')
    # Retirer www.
    url = url.replace('www.', '')
    # Retirer les doubles slashes (problÃ¨me frÃ©quent)
    while '//' in url:
        url = url.replace('//', '/')
    # Retirer le slash final
    url = url.rstrip('/')
    # Retirer le slash initial si prÃ©sent
    url = url.lstrip('/')
    return url

# =============================================================================
# INTERFACE
# =============================================================================

st.title("ğŸ“Š Haloscan SEO Diff Analyzer")

with st.sidebar:
    st.header("ğŸ“ Import des donnÃ©es")
    
    st.subheader("ğŸ“Š Fichiers Haloscan")
    uploaded_file_p1 = st.file_uploader("1ï¸âƒ£ CSV Haloscan PÃ©riode 1", type=['csv'], key="haloscan_p1")
    uploaded_file_p2 = st.file_uploader("2ï¸âƒ£ CSV Haloscan PÃ©riode 2", type=['csv'], key="haloscan_p2")
    
    # Labels des pÃ©riodes (personnalisables)
    if uploaded_file_p1 and uploaded_file_p2:
        st.caption("ğŸ“… Nommez vos pÃ©riodes :")
        col1, col2 = st.columns(2)
        with col1:
            label_debut_p1 = st.text_input("DÃ©but P1", value="Jan 2025", key="label_debut_p1")
            label_fin_p1 = st.text_input("Fin P1 / DÃ©but P2", value="Sept 2025", key="label_fin_p1")
        with col2:
            label_fin_p2 = st.text_input("Fin P2", value="FÃ©v 2026", key="label_fin_p2")
    else:
        label_debut_p1 = "DÃ©but P1"
        label_fin_p1 = "Fin P1"
        label_fin_p2 = "Fin P2"
    
    st.subheader("ğŸ’° DonnÃ©es business")
    uploaded_leads = st.file_uploader("3ï¸âƒ£ Excel Leads par URL (optionnel)", type=['xlsx', 'xls'], 
                                       help="Fichier avec colonnes: url, puis une colonne par mois (YYYY_MM)")
    
    st.subheader("ğŸ” Search Console")
    uploaded_gsc = st.file_uploader("4ï¸âƒ£ ZIP Search Console (optionnel)", type=['zip'],
                                     help="Export ZIP de Google Search Console (Performance)")
    
    st.subheader("ğŸ¤– Analyse IA")
    anthropic_api_key = st.text_input("ClÃ© API Anthropic", type="password", 
                                       help="Pour gÃ©nÃ©rer l'analyse IA du rapport")

# Variables globales pour les leads
leads_df = None
has_leads = False
month_cols = []
periode_avant = []
periode_apres = []

# Variables pour le mode multi-pÃ©riodes
has_dual_haloscan = False
df_p1 = None
df_p2 = None

# Variables pour Search Console
gsc_queries_df = None
gsc_pages_df = None
has_gsc = False

if uploaded_leads:
    # Lire la feuille "Leads totaux par urls" (pas la premiÃ¨re feuille qui contient les visites)
    try:
        xlsx = pd.ExcelFile(uploaded_leads)
        
        # Afficher les feuilles disponibles
        st.sidebar.caption(f"Feuilles : {xlsx.sheet_names}")
        
        # Chercher la feuille des leads par son nom exact ou contenant "lead"
        leads_sheet = None
        for sheet in xlsx.sheet_names:
            if 'lead' in sheet.lower():
                leads_sheet = sheet
                break
        
        if leads_sheet:
            leads_df_raw = pd.read_excel(xlsx, sheet_name=leads_sheet)
            st.sidebar.success(f"ğŸ“Š Feuille chargÃ©e : {leads_sheet} ({len(leads_df_raw)} lignes)")
        else:
            # IMPORTANT: La feuille des leads est gÃ©nÃ©ralement la 2Ã¨me (index 1)
            # La 1Ã¨re feuille (index 0) contient les visites
            if len(xlsx.sheet_names) > 1:
                leads_df_raw = pd.read_excel(xlsx, sheet_name=1)
                st.sidebar.success(f"ğŸ“Š Feuille chargÃ©e : {xlsx.sheet_names[1]} ({len(leads_df_raw)} lignes)")
            else:
                leads_df_raw = pd.read_excel(xlsx, sheet_name=0)
                st.sidebar.warning(f"âš ï¸ Une seule feuille : {xlsx.sheet_names[0]}")
        
        # VÃ‰RIFICATION : Les leads doivent avoir des valeurs faibles (< 1000 en gÃ©nÃ©ral)
        # Si la moyenne est > 500, c'est probablement les visites
        month_cols_check = [c for c in leads_df_raw.columns if '2025' in str(c) or '2024' in str(c)]
        if month_cols_check:
            mean_val = leads_df_raw[month_cols_check].mean().mean()
            if mean_val > 500:
                st.sidebar.error(f"âš ï¸ ATTENTION : Moyenne = {mean_val:.0f} â€” Ce sont probablement les VISITES, pas les leads !")
                st.sidebar.info("VÃ©rifiez que la feuille 'Leads totaux par urls' est bien dans le fichier")
            else:
                st.sidebar.info(f"âœ… Moyenne = {mean_val:.1f} â€” DonnÃ©es leads OK")
        
        # Debug : afficher un aperÃ§u pour confirmer
        with st.sidebar.expander("ğŸ” VÃ©rification donnÃ©es leads", expanded=False):
            st.write(f"Feuilles disponibles : {xlsx.sheet_names}")
            st.write(f"Lignes : {len(leads_df_raw)}")
            # Trouver une colonne de mois pour montrer un exemple
            sample_cols = [c for c in leads_df_raw.columns if '2025' in str(c)][:2]
            if sample_cols and 'url' in leads_df_raw.columns:
                st.write(f"Exemple (premiÃ¨res lignes) :")
                st.dataframe(leads_df_raw[['url'] + sample_cols].head(3))
                
    except Exception as e:
        leads_df_raw = pd.read_excel(uploaded_leads)
        st.sidebar.warning(f"Lecture par dÃ©faut (erreur: {e})")
    
    # Identifier les colonnes de mois
    month_cols = [col for col in leads_df_raw.columns if col != 'url' and '_' in str(col)]
    month_cols_sorted = sorted(month_cols)
    
    has_leads = True
    
    # Fonction pour convertir un label ("Jan 2025", "Sept 2025") en format YYYY_MM
    def label_to_month_format(label):
        """Convertit 'Jan 2025' ou 'Janvier 2025' en '2025_01'"""
        mois_map = {
            'jan': '01', 'fev': '02', 'fÃ©v': '02', 'mar': '03', 'avr': '04', 'apr': '04',
            'mai': '05', 'may': '05', 'jun': '06', 'jui': '07', 'jul': '07',
            'aou': '08', 'aoÃ»': '08', 'aug': '08', 'sep': '09', 'oct': '10', 
            'nov': '11', 'dec': '12', 'dÃ©c': '12'
        }
        label_lower = label.lower().strip()
        year = None
        month = None
        
        # Extraire l'annÃ©e (4 chiffres)
        import re
        year_match = re.search(r'20\d{2}', label_lower)
        if year_match:
            year = year_match.group()
        
        # Extraire le mois
        for mois_key, mois_val in mois_map.items():
            if mois_key in label_lower:
                month = mois_val
                break
        
        if year and month:
            return f"{year}_{month}"
        return None
    
    # DÃ©tecter automatiquement les pÃ©riodes si labels Haloscan sont dÃ©finis
    auto_detected = False
    if uploaded_file_p1 and uploaded_file_p2:
        debut_p1_month = label_to_month_format(label_debut_p1)
        fin_p1_month = label_to_month_format(label_fin_p1)
        fin_p2_month = label_to_month_format(label_fin_p2)
        
        if debut_p1_month and fin_p1_month and fin_p2_month:
            # PÃ©riode AVANT = du dÃ©but P1 jusqu'Ã  (fin P1 - 1 mois)
            # PÃ©riode APRÃˆS = de fin P1 jusqu'Ã  fin P2
            default_avant = [m for m in month_cols_sorted if debut_p1_month <= m < fin_p1_month]
            default_apres = [m for m in month_cols_sorted if fin_p1_month <= m <= fin_p2_month]
            
            if default_avant and default_apres:
                auto_detected = True
                st.sidebar.success(f"ğŸ¯ PÃ©riodes auto-dÃ©tectÃ©es depuis labels Haloscan")
    
    # Si pas de dÃ©tection auto, utiliser les valeurs par dÃ©faut
    if not auto_detected:
        default_avant = [c for c in month_cols_sorted if c.startswith('2025_09')]
        if not default_avant:
            default_avant = month_cols_sorted[-6:-3] if len(month_cols_sorted) >= 6 else month_cols_sorted[:3]
        
        default_apres = [c for c in month_cols_sorted if c.startswith('2025_11') or c.startswith('2026')]
        if not default_apres:
            default_apres = month_cols_sorted[-3:] if len(month_cols_sorted) >= 3 else month_cols_sorted[-1:]
    
    with st.sidebar:
        st.subheader("ğŸ“… PÃ©riodes leads Ã  comparer")
        if auto_detected:
            st.caption(f"BasÃ© sur vos labels : {label_debut_p1} â†’ {label_fin_p1} â†’ {label_fin_p2}")
        else:
            st.caption("SÃ©lectionnez les mois correspondant Ã  votre export Haloscan")
        
        # PÃ©riode AVANT (ancienne position)
        st.markdown("**PÃ©riode AVANT** (dÃ©but analyse)")
        periode_avant = st.multiselect(
            "Mois pÃ©riode avant",
            options=month_cols_sorted,
            default=default_avant,
            key="avant"
        )
        
        # PÃ©riode APRÃˆS (position actuelle)
        st.markdown("**PÃ©riode APRÃˆS** (fin analyse)")
        periode_apres = st.multiselect(
            "Mois pÃ©riode aprÃ¨s", 
            options=month_cols_sorted,
            default=default_apres,
            key="apres"
        )
    
    # Calculer les mÃ©triques leads sur les bonnes pÃ©riodes
    leads_df = leads_df_raw.copy()
    
    # S'assurer que les colonnes de mois sont numÃ©riques
    for col in month_cols:
        if col in leads_df.columns:
            leads_df[col] = pd.to_numeric(leads_df[col], errors='coerce').fillna(0)
    
    # CrÃ©er les noms de colonnes dynamiques basÃ©s sur la sÃ©lection
    periode_avant_label = '+'.join(periode_avant) if periode_avant else 'N/A'
    periode_apres_label = '+'.join(periode_apres) if periode_apres else 'N/A'
    
    # Calculer les totaux sur TOUS les mois entre le dÃ©but de pÃ©riode AVANT et la fin de pÃ©riode APRÃˆS
    if periode_avant and periode_apres:
        # Trouver le mois min (dÃ©but pÃ©riode) et max (fin pÃ©riode)
        all_selected = periode_avant + periode_apres
        mois_min = min(all_selected)
        mois_max = max(all_selected)
        
        # Filtrer les colonnes de mois qui sont dans cette plage
        periode_complete = [m for m in month_cols_sorted if mois_min <= m <= mois_max]
        
        if periode_complete:
            leads_df['leads_total'] = leads_df[periode_complete].sum(axis=1)
            st.sidebar.caption(f"ğŸ“Š Leads total : {mois_min} â†’ {mois_max} ({len(periode_complete)} mois)")
        else:
            leads_df['leads_total'] = 0
    elif month_cols:
        leads_df['leads_total'] = leads_df[month_cols].sum(axis=1)
    else:
        leads_df['leads_total'] = 0
        
    leads_df['leads_avant'] = leads_df[periode_avant].sum(axis=1) if periode_avant else 0
    leads_df['leads_apres'] = leads_df[periode_apres].sum(axis=1) if periode_apres else 0
    leads_df['leads_evolution'] = leads_df['leads_apres'] - leads_df['leads_avant']
    leads_df['leads_evolution_pct'] = ((leads_df['leads_apres'] - leads_df['leads_avant']) / leads_df['leads_avant'].replace(0, 1) * 100).round(1)
    
    leads_df['url_normalized'] = leads_df['url'].apply(normalize_url)
    
    st.sidebar.success(f"âœ… {len(leads_df):,} URLs avec donnÃ©es leads")
    if periode_avant and periode_apres:
        st.sidebar.info(f"Comparaison : {periode_avant_label} â†’ {periode_apres_label}")

# Charger Search Console si uploadÃ©
if uploaded_gsc:
    import zipfile
    import io
    
    try:
        with zipfile.ZipFile(uploaded_gsc, 'r') as z:
            # Chercher les fichiers RequÃªtes et Pages
            files_in_zip = z.namelist()
            
            queries_file = None
            pages_file = None
            
            for f in files_in_zip:
                if 'RequÃªtes' in f or 'Queries' in f or 'requetes' in f.lower():
                    queries_file = f
                elif 'Pages' in f or 'pages' in f.lower():
                    pages_file = f
            
            # Charger RequÃªtes
            if queries_file:
                with z.open(queries_file) as qf:
                    gsc_queries_df = pd.read_csv(qf)
                    # Normaliser les noms de colonnes
                    gsc_queries_df.columns = gsc_queries_df.columns.str.strip()
                    # Renommer la premiÃ¨re colonne en 'query'
                    first_col = gsc_queries_df.columns[0]
                    gsc_queries_df = gsc_queries_df.rename(columns={first_col: 'query'})
                    # Convertir CTR en float
                    if 'CTR' in gsc_queries_df.columns:
                        gsc_queries_df['CTR'] = gsc_queries_df['CTR'].astype(str).str.replace('%', '').str.replace(',', '.').astype(float)
                    # Normaliser les requÃªtes pour le matching
                    gsc_queries_df['query_normalized'] = gsc_queries_df['query'].str.lower().str.strip()
            
            # Charger Pages
            if pages_file:
                with z.open(pages_file) as pf:
                    gsc_pages_df = pd.read_csv(pf)
                    # Normaliser les noms de colonnes
                    gsc_pages_df.columns = gsc_pages_df.columns.str.strip()
                    # Renommer la premiÃ¨re colonne en 'url'
                    first_col = gsc_pages_df.columns[0]
                    gsc_pages_df = gsc_pages_df.rename(columns={first_col: 'url'})
                    # Convertir CTR en float
                    if 'CTR' in gsc_pages_df.columns:
                        gsc_pages_df['CTR'] = gsc_pages_df['CTR'].astype(str).str.replace('%', '').str.replace(',', '.').astype(float)
                    # Normaliser les URLs pour le matching
                    gsc_pages_df['url_normalized'] = gsc_pages_df['url'].apply(normalize_url)
            
            if gsc_queries_df is not None or gsc_pages_df is not None:
                has_gsc = True
                gsc_info = []
                if gsc_queries_df is not None:
                    gsc_info.append(f"{len(gsc_queries_df):,} requÃªtes")
                if gsc_pages_df is not None:
                    gsc_info.append(f"{len(gsc_pages_df):,} pages")
                st.sidebar.success(f"ğŸ” GSC : {' | '.join(gsc_info)}")
            else:
                st.sidebar.warning("âš ï¸ Fichiers RequÃªtes/Pages non trouvÃ©s dans le ZIP")
                
    except Exception as e:
        st.sidebar.error(f"âŒ Erreur lecture ZIP GSC: {e}")

# DÃ©terminer le mode de fonctionnement
uploaded_file = None
if uploaded_file_p1 and uploaded_file_p2:
    # Mode double pÃ©riode
    has_dual_haloscan = True
    st.sidebar.success("ğŸ“Š Mode double pÃ©riode activÃ©")
elif uploaded_file_p1:
    # Mode simple avec P1
    uploaded_file = uploaded_file_p1
elif uploaded_file_p2:
    # Mode simple avec P2
    uploaded_file = uploaded_file_p2

# Charger et fusionner les donnÃ©es si mode double pÃ©riode
if has_dual_haloscan:
    df_p1 = load_data(uploaded_file_p1)
    df_p2 = load_data(uploaded_file_p2)
    
    # Renommer les colonnes de position pour P1
    df_p1 = df_p1.rename(columns={
        'ancienne_pos': 'pos_debut_p1',
        'derniere_pos': 'pos_fin_p1',
        'diff_pos': 'diff_p1'
    })
    
    # Renommer les colonnes de position pour P2
    df_p2 = df_p2.rename(columns={
        'ancienne_pos': 'pos_debut_p2',
        'derniere_pos': 'pos_fin_p2',
        'diff_pos': 'diff_p2'
    })
    
    # Fusionner sur mot_cle + url
    df = df_p1.merge(
        df_p2[['mot_cle', 'url', 'pos_debut_p2', 'pos_fin_p2', 'diff_p2']],
        on=['mot_cle', 'url'],
        how='outer',
        suffixes=('', '_p2')
    )
    
    # Calculer les colonnes consolidÃ©es
    # Position de dÃ©part = pos_debut_p1 (ou pos_debut_p2 si pas de P1)
    df['ancienne_pos'] = df['pos_debut_p1'].fillna(df['pos_debut_p2'])
    # Position finale = pos_fin_p2 (ou pos_fin_p1 si pas de P2)
    df['derniere_pos'] = df['pos_fin_p2'].fillna(df['pos_fin_p1'])
    # Diff totale : positif = gain (ancienne - derniÃ¨re, car passer de 96 Ã  1 = +95)
    df['diff_pos'] = df['ancienne_pos'] - df['derniere_pos']
    
    # Calculer la tendance multi-pÃ©riode
    def calc_tendance_multi(row):
        d1 = row.get('diff_p1', 0) or 0
        d2 = row.get('diff_p2', 0) or 0
        
        if pd.isna(d1): d1 = 0
        if pd.isna(d2): d2 = 0
        
        if d1 < -5 and d2 < -5:
            return "ğŸ“‰ğŸ“‰ Chute continue"
        elif d1 > 5 and d2 < -5:
            return "ğŸ“ˆğŸ“‰ Rebond puis rechute"
        elif d1 < -5 and d2 > 5:
            return "ğŸ“‰ğŸ“ˆ RÃ©cupÃ©ration"
        elif d1 > 5 and d2 > 5:
            return "ğŸ“ˆğŸ“ˆ Hausse continue"
        elif abs(d1) <= 5 and abs(d2) <= 5:
            return "â¡ï¸ Stable"
        elif d1 < 0 or d2 < 0:
            return "ğŸ“‰ Baisse"
        else:
            return "ğŸ“ˆ Hausse"
    
    df['tendance_multi'] = df.apply(calc_tendance_multi, axis=1)
    
    # Recalculer le volume si nÃ©cessaire
    if 'volume' not in df.columns and 'volumeh' in df.columns:
        df['volume'] = df['volumeh']
    
    # Recalculer priority_score
    if 'volume' in df.columns:
        df['priority_score'] = df['volume'].fillna(0) * df['diff_pos'].abs().fillna(0)
    else:
        df['priority_score'] = df['diff_pos'].abs().fillna(0)
    
    st.sidebar.info(f"ğŸ”— {len(df):,} KW fusionnÃ©s (P1: {len(df_p1):,} | P2: {len(df_p2):,})")

elif uploaded_file:
    df = load_data(uploaded_file)
    has_dual_haloscan = False

# Suite du traitement si on a des donnÃ©es
if (has_dual_haloscan or uploaded_file) and 'df' in dir():
    
    # Croiser avec les donnÃ©es leads si disponibles
    if has_leads and 'url' in df.columns:
        df['url_normalized'] = df['url'].apply(normalize_url)
        df = df.merge(
            leads_df[['url_normalized', 'leads_total', 'leads_avant', 'leads_apres', 'leads_evolution', 'leads_evolution_pct']], 
            on='url_normalized', 
            how='left'
        )
        
        # Stocker les labels de pÃ©riode pour l'affichage
        df.attrs['periode_avant_label'] = periode_avant_label
        df.attrs['periode_apres_label'] = periode_apres_label
        
        # CrÃ©er indicateur visuel de tendance leads
        def tendance_leads(row):
            evol = row.get('leads_evolution', 0) or 0
            pct = row.get('leads_evolution_pct', 0) or 0
            if evol < -10 or pct < -20:
                return "ğŸ”»ğŸ”» CHUTE"
            elif evol < 0:
                return "ğŸ”» Baisse"
            elif evol == 0:
                return "â¡ï¸ Stable"
            elif evol > 10 or pct > 20:
                return "ğŸ”ºğŸ”º BOOM"
            else:
                return "ğŸ”º Hausse"
        
        df['tendance_leads'] = df.apply(tendance_leads, axis=1)
        
        # Score de prioritÃ© enrichi : 
        # - priority_score = volume recherche Ã— |diff_pos|
        # - On booste si l'URL gÃ©nÃ¨re des leads (leads_total)
        # - On booste ENCORE PLUS si les leads sont en baisse (leads_evolution < 0)
        base_score = df['priority_score']
        leads_boost = (1 + df['leads_total'].fillna(0) / 100)  # Plus de leads = plus important
        
        # Malus si les leads baissent (Ã©volution nÃ©gative)
        leads_trend = df['leads_evolution'].fillna(0)
        trend_multiplier = 1 + (leads_trend.clip(upper=0).abs() / 100)  # Perte de leads = urgence
        
        df['priority_score_business'] = base_score * leads_boost * trend_multiplier
        
        # Flag pour identifier les URLs en double peine (perte SEO + perte leads)
        df['double_peine'] = (df['diff_pos'] < 0) & (df['leads_evolution'] < 0)
        
        # CrÃ©er indicateur visuel de tendance SEO (positions)
        def tendance_seo(diff):
            if pd.isna(diff):
                return "â¡ï¸ N/A"
            diff = int(diff)
            if diff <= -20:
                return "ğŸ”»ğŸ”» CHUTE"
            elif diff < 0:
                return "ğŸ”» Baisse"
            elif diff == 0:
                return "â¡ï¸ Stable"
            elif diff >= 20:
                return "ğŸ”ºğŸ”º BOOM"
            else:
                return "ğŸ”º Hausse"
        
        df['tendance_seo'] = df['diff_pos'].apply(tendance_seo)
        
        st.success(f"âœ… {len(df):,} mots-clÃ©s chargÃ©s â€” DonnÃ©es leads croisÃ©es !")
        
        # Stats de matching
        urls_avec_leads = df[df['leads_total'].notna() & (df['leads_total'] > 0)]['url'].nunique()
        urls_double_peine = df[df['double_peine'] == True]['url'].nunique()
        st.info(f"ğŸ“Š {urls_avec_leads} URLs avec leads | âš ï¸ {urls_double_peine} URLs en double peine (perte SEO + perte leads)")
        
        has_leads_merged = True
    else:
        df['leads_total'] = 0
        df['leads_avant'] = 0
        df['leads_apres'] = 0
        df['leads_evolution'] = 0
        df['tendance_leads'] = "â¡ï¸ N/A"
        
        # CrÃ©er indicateur visuel de tendance SEO (positions) mÃªme sans leads
        def tendance_seo(diff):
            if pd.isna(diff):
                return "â¡ï¸ N/A"
            diff = int(diff)
            if diff <= -20:
                return "ğŸ”»ğŸ”» CHUTE"
            elif diff < 0:
                return "ğŸ”» Baisse"
            elif diff == 0:
                return "â¡ï¸ Stable"
            elif diff >= 20:
                return "ğŸ”ºğŸ”º BOOM"
            else:
                return "ğŸ”º Hausse"
        
        df['tendance_seo'] = df['diff_pos'].apply(tendance_seo)
        
        if has_leads:
            st.warning("âš ï¸ Fichier leads chargÃ© mais colonne 'url' manquante dans le CSV Haloscan")
        st.success(f"âœ… {len(df):,} mots-clÃ©s chargÃ©s")
        has_leads_merged = False
    
    # Debug colonnes
    with st.sidebar:
        with st.expander("ğŸ” Colonnes", expanded=True):
            st.write(list(df.columns))
    
    # VÃ©rification diff_pos
    if 'diff_pos' not in df.columns:
        st.error(f"âŒ Colonne 'diff_pos' non trouvÃ©e. Colonnes: {list(df.columns)}")
        st.stop()
    
    # ==========================================================================
    # FILTRES
    # ==========================================================================
    
    with st.sidebar:
        st.header("ğŸ›ï¸ Filtres")
        
        variation = st.multiselect("Variation", ['Pertes', 'Gains', 'Stables'], default=['Pertes', 'Gains', 'Stables'])
        
        if 'volume' in df.columns:
            vmin, vmax = int(df['volume'].min() or 0), int(df['volume'].max() or 10000)
            vol_range = st.slider("Volume", vmin, vmax, (vmin, vmax))
        else:
            vol_range = None
        
        search_kw = st.text_input("ğŸ” Mot-clÃ©")
        search_url = st.text_input("ğŸ” URL contient")
    
    # Appliquer filtres
    df_f = df.copy()
    
    # Filtre variation
    masks = []
    if 'Pertes' in variation:
        masks.append(df_f['diff_pos'] < 0)
    if 'Gains' in variation:
        masks.append(df_f['diff_pos'] > 0)
    if 'Stables' in variation:
        masks.append(df_f['diff_pos'] == 0)
    if masks:
        combined = masks[0]
        for m in masks[1:]:
            combined = combined | m
        df_f = df_f[combined]
    
    # Filtre volume
    if vol_range and 'volume' in df_f.columns:
        df_f = df_f[(df_f['volume'] >= vol_range[0]) & (df_f['volume'] <= vol_range[1])]
    
    # Filtre recherche
    if search_kw and 'mot_cle' in df_f.columns:
        df_f = df_f[df_f['mot_cle'].astype(str).str.contains(search_kw, case=False, na=False)]
    if search_url and 'url' in df_f.columns:
        df_f = df_f[df_f['url'].astype(str).str.contains(search_url, case=False, na=False)]
    
    # ==========================================================================
    # KPIs
    # ==========================================================================
    
    total = len(df_f)
    pertes = len(df_f[df_f['diff_pos'] < 0])
    gains = len(df_f[df_f['diff_pos'] > 0])
    stables = len(df_f[df_f['diff_pos'] == 0])
    
    vol_perdu = int(df_f[df_f['diff_pos'] < 0]['volume'].fillna(0).sum()) if 'volume' in df_f.columns else 0
    vol_gagne = int(df_f[df_f['diff_pos'] > 0]['volume'].fillna(0).sum()) if 'volume' in df_f.columns else 0
    
    # ==========================================================================
    # ONGLETS
    # ==========================================================================
    
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs(["ğŸ“Š Dashboard", "ğŸ”´ Pertes", "ğŸ“ Par URL", "ğŸŸ¢ Gains", "ğŸ”„ Cannibalisation", "ğŸ” Search Console", "ğŸ“ Rapport"])
    
    # TAB 1: DASHBOARD
    with tab1:
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Total", f"{total:,}")
        c2.metric("ğŸ”´ Pertes", f"{pertes:,}")
        c3.metric("ğŸŸ¢ Gains", f"{gains:,}")
        c4.metric("âšª Stables", f"{stables:,}")
        
        st.divider()
        
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("ğŸ“‰ Volume perdu", f"{vol_perdu:,}")
        c2.metric("ğŸ“ˆ Volume gagnÃ©", f"{vol_gagne:,}")
        
        # MÃ©triques leads si disponibles
        if has_leads_merged:
            # Leads sur les URLs en perte - NE PAS compter plusieurs fois la mÃªme URL
            df_pertes_dash = df_f[df_f['diff_pos'] < 0]
            df_urls_perte_unique = df_pertes_dash.drop_duplicates(subset=['url']) if 'url' in df_pertes_dash.columns else df_pertes_dash
            
            leads_urls_perte = df_urls_perte_unique['leads_total'].fillna(0).sum()
            c3.metric("âš ï¸ Leads sur URLs en perte", f"{int(leads_urls_perte):,}")
            
            leads_evol = df_urls_perte_unique['leads_evolution'].fillna(0).sum()
            delta_color = "inverse" if leads_evol < 0 else "normal"
            c4.metric("ğŸ“Š Ã‰vol. leads (pÃ©riode)", f"{int(leads_evol):+,}", delta_color=delta_color)
        
        # Section MULTI-PÃ‰RIODES si disponible
        if has_dual_haloscan and 'tendance_multi' in df_f.columns:
            st.divider()
            st.subheader(f"ğŸ“ˆ Analyse multi-pÃ©riodes ({label_debut_p1} â†’ {label_fin_p1} â†’ {label_fin_p2})")
            
            # Compter les tendances
            tendances = df_f['tendance_multi'].value_counts()
            
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("ğŸ“‰ğŸ“‰ Chute continue", f"{tendances.get('ğŸ“‰ğŸ“‰ Chute continue', 0):,}", help="Perte P1 ET perte P2")
            col2.metric("ğŸ“ˆğŸ“‰ Rebond puis rechute", f"{tendances.get('ğŸ“ˆğŸ“‰ Rebond puis rechute', 0):,}", help="Gain P1 puis perte P2")
            col3.metric("ğŸ“‰ğŸ“ˆ RÃ©cupÃ©ration", f"{tendances.get('ğŸ“‰ğŸ“ˆ RÃ©cupÃ©ration', 0):,}", help="Perte P1 puis gain P2")
            col4.metric("ğŸ“ˆğŸ“ˆ Hausse continue", f"{tendances.get('ğŸ“ˆğŸ“ˆ Hausse continue', 0):,}", help="Gain P1 ET gain P2")
            
            # Tableau des KW en chute continue (prioritÃ© max)
            df_chute_continue = df_f[df_f['tendance_multi'] == 'ğŸ“‰ğŸ“‰ Chute continue'].copy()
            if len(df_chute_continue) > 0:
                st.error(f"ğŸš¨ **{len(df_chute_continue):,}** mots-clÃ©s en CHUTE CONTINUE â€” ProblÃ¨me structurel Ã  traiter !")
                
                # Afficher les colonnes pertinentes
                cols_multi = ['mot_cle', 'url', 'pos_debut_p1', 'pos_fin_p1', 'diff_p1', 'pos_fin_p2', 'diff_p2', 'diff_pos', 'volume']
                cols_multi = [c for c in cols_multi if c in df_chute_continue.columns]
                
                # Renommer pour clartÃ© avec labels dynamiques
                df_chute_display = df_chute_continue[cols_multi].head(50).copy()
                rename_map = {
                    'pos_debut_p1': f'Pos {label_debut_p1}',
                    'pos_fin_p1': f'Pos {label_fin_p1}',
                    'diff_p1': f'Î” P1',
                    'pos_fin_p2': f'Pos {label_fin_p2}',
                    'diff_p2': f'Î” P2',
                    'diff_pos': 'Î” TOTAL',
                    'volume': 'Volume'
                }
                df_chute_display = df_chute_display.rename(columns=rename_map)
                
                st.dataframe(df_chute_display.sort_values('Î” TOTAL', ascending=True), use_container_width=True, height=300)
        
        # Section DOUBLE PEINE (suite du code existant)
        if has_leads_merged:
            if 'double_peine' in df_f.columns:
                df_double_peine = df_f[df_f['double_peine'] == True]
                if len(df_double_peine) > 0:
                    st.divider()
                    st.subheader("ğŸš¨ ALERTE : URLs en DOUBLE PEINE (perte SEO + perte leads)")
                    st.error(f"**{df_double_peine['url'].nunique()}** URLs perdent Ã  la fois des positions ET des leads !")
                    
                    # RÃ©cupÃ©rer les labels de pÃ©riode
                    p_avant = df.attrs.get('periode_avant_label', 'AVANT')
                    p_apres = df.attrs.get('periode_apres_label', 'APRÃˆS')
                    
                    # Tableau des URLs double peine
                    agg_dp = {'diff_pos': ['count', 'sum']}
                    if 'tendance_seo' in df_double_peine.columns:
                        agg_dp['tendance_seo'] = lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else "â¡ï¸ N/A"
                    if 'leads_avant' in df_double_peine.columns:
                        agg_dp['leads_avant'] = 'first'
                    if 'leads_apres' in df_double_peine.columns:
                        agg_dp['leads_apres'] = 'first'
                    if 'leads_evolution' in df_double_peine.columns:
                        agg_dp['leads_evolution'] = 'first'
                    if 'tendance_leads' in df_double_peine.columns:
                        agg_dp['tendance_leads'] = 'first'
                    
                    df_dp_urls = df_double_peine.groupby('url').agg(agg_dp).reset_index()
                    df_dp_urls.columns = ['URL', 'KW perdus', 'Diff total'] + \
                                        (['ğŸ“Š SEO'] if 'tendance_seo' in df_double_peine.columns else []) + \
                                        ([f'Leads {p_avant}'] if 'leads_avant' in df_double_peine.columns else []) + \
                                        ([f'Leads {p_apres}'] if 'leads_apres' in df_double_peine.columns else []) + \
                                        (['Ã‰vol. Leads'] if 'leads_evolution' in df_double_peine.columns else []) + \
                                        (['ğŸ“Š LEADS'] if 'tendance_leads' in df_double_peine.columns else [])
                    
                    # Trier par Ã©volution leads (les plus grosses pertes en premier)
                    if 'Ã‰vol. Leads' in df_dp_urls.columns:
                        df_dp_urls = df_dp_urls.sort_values('Ã‰vol. Leads', ascending=True)
                    elif 'Diff total' in df_dp_urls.columns:
                        df_dp_urls = df_dp_urls.sort_values('Diff total', ascending=True)
                    
                    st.dataframe(df_dp_urls.head(20), use_container_width=True, hide_index=True)
        
        st.divider()
        
        col1, col2 = st.columns(2)
        with col1:
            fig = px.pie(values=[pertes, gains, stables], names=['Pertes', 'Gains', 'Stables'],
                        color_discrete_sequence=['#EF4444', '#22C55E', '#6B7280'])
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = px.histogram(df_f, x='diff_pos', nbins=50)
            st.plotly_chart(fig, use_container_width=True)
        
        # Top URLs impactÃ©es avec leads
        try:
            df_pertes_temp = df_f[df_f['diff_pos'] < 0]
            if has_leads_merged and len(df_pertes_temp) > 0 and 'leads_total' in df_pertes_temp.columns:
                st.subheader("ğŸ¯ URLs critiques : Pertes SEO + Impact Business")
                
                # Construire l'agrÃ©gation dynamiquement selon les colonnes disponibles
                agg_dict_dash = {'diff_pos': ('diff_pos', 'count')}
                if 'volume' in df_pertes_temp.columns:
                    agg_dict_dash['volume_perdu'] = ('volume', 'sum')
                if 'leads_total' in df_pertes_temp.columns:
                    agg_dict_dash['leads_total'] = ('leads_total', 'first')
                if 'leads_evolution' in df_pertes_temp.columns:
                    agg_dict_dash['leads_evolution'] = ('leads_evolution', 'first')
                if 'tendance_leads' in df_pertes_temp.columns:
                    agg_dict_dash['tendance_leads'] = ('tendance_leads', 'first')
                
                df_perte_urls = df_pertes_temp.groupby('url').agg(**agg_dict_dash).reset_index()
                df_perte_urls = df_perte_urls.rename(columns={'diff_pos': 'kw_perdus'})
                
                # Trier par Ã©volution leads (les plus grosses pertes en premier)
                if 'leads_evolution' in df_perte_urls.columns:
                    df_perte_urls = df_perte_urls.sort_values('leads_evolution', ascending=True).head(15)
                else:
                    df_perte_urls = df_perte_urls.sort_values('kw_perdus', ascending=False).head(15)
                
                st.dataframe(df_perte_urls, use_container_width=True)
        except Exception as e:
            st.warning(f"Impossible d'afficher les URLs critiques: {e}")
    
    # TAB 2: PERTES
    with tab2:
        st.header("ğŸ”´ Pertes critiques")
        df_pertes = df_f[df_f['diff_pos'] < 0].sort_values('diff_pos', ascending=True)
        st.info(f"**{len(df_pertes):,}** mots-clÃ©s en perte")
        
        cols = [c for c in ['mot_cle', 'url', 'ancienne_pos', 'derniere_pos', 'diff_pos', 'tendance_seo', 'volume'] if c in df_pertes.columns]
        st.dataframe(df_pertes[cols], use_container_width=True, height=600)
        
        csv = df_pertes[cols].to_csv(index=False, sep=';').encode('utf-8')
        st.download_button("ğŸ“¥ Export CSV", csv, "pertes.csv")
    
    # TAB 3: PAR URL
    with tab3:
        st.header("ğŸ“ Analyse par URL")
        if 'url' in df_f.columns:
            try:
                # Construire l'agrÃ©gation dynamiquement
                agg_funcs = {
                    'diff_pos': ['count', lambda x: (x < 0).sum(), lambda x: (x > 0).sum(), 'sum'],
                }
                if 'volume' in df_f.columns:
                    agg_funcs['volume'] = 'sum'
                if has_leads_merged:
                    if 'leads_total' in df_f.columns:
                        agg_funcs['leads_total'] = 'first'
                    if 'leads_avant' in df_f.columns:
                        agg_funcs['leads_avant'] = 'first'
                    if 'leads_apres' in df_f.columns:
                        agg_funcs['leads_apres'] = 'first'
                    if 'leads_evolution' in df_f.columns:
                        agg_funcs['leads_evolution'] = 'first'
                    if 'tendance_leads' in df_f.columns:
                        agg_funcs['tendance_leads'] = 'first'
                
                url_stats = df_f.groupby('url').agg(agg_funcs).reset_index()
                
                # Aplatir les colonnes multi-index
                new_cols = ['url']
                for col in url_stats.columns[1:]:
                    if isinstance(col, tuple):
                        new_cols.append(f"{col[0]}_{col[1]}" if col[1] != 'sum' and col[1] != 'first' else col[0])
                    else:
                        new_cols.append(col)
                url_stats.columns = new_cols
                
                # Renommer les colonnes pour plus de clartÃ©
                rename_dict = {
                    'diff_pos_count': 'total_kw',
                    'diff_pos_<lambda_0>': 'kw_perte', 
                    'diff_pos_<lambda_1>': 'kw_gain',
                    'diff_pos_sum': 'diff_total'
                }
                url_stats = url_stats.rename(columns=rename_dict)
                
                # Ajouter indicateur tendance SEO basÃ© sur diff_total
                def tendance_seo_url(diff):
                    if pd.isna(diff):
                        return "â¡ï¸ N/A"
                    diff = int(diff)
                    if diff <= -50:
                        return "ğŸ”»ğŸ”» CHUTE"
                    elif diff < 0:
                        return "ğŸ”» Baisse"
                    elif diff == 0:
                        return "â¡ï¸ Stable"
                    elif diff >= 50:
                        return "ğŸ”ºğŸ”º BOOM"
                    else:
                        return "ğŸ”º Hausse"
                
                if 'diff_total' in url_stats.columns:
                    url_stats['ğŸ“Š SEO'] = url_stats['diff_total'].apply(tendance_seo_url)
                
                # Ajouter tendance leads si dispo
                if 'tendance_leads' in url_stats.columns:
                    url_stats = url_stats.rename(columns={'tendance_leads': 'ğŸ“Š LEADS'})
                
                # Tri par Ã©volution leads ou par nombre de KW en perte
                if 'leads_evolution' in url_stats.columns:
                    url_stats = url_stats.sort_values('leads_evolution', ascending=True)
                elif 'kw_perte' in url_stats.columns:
                    url_stats = url_stats.sort_values('kw_perte', ascending=False)
                else:
                    url_stats = url_stats.sort_values('total_kw', ascending=False)
                
                st.info(f"**{len(url_stats):,}** URLs analysÃ©es")
                st.dataframe(url_stats, use_container_width=True, height=500)
                
                # Export
                csv_urls = url_stats.to_csv(index=False, sep=';').encode('utf-8')
                st.download_button("ğŸ“¥ Exporter TOUTES les URLs (CSV)", csv_urls, "analyse_urls_complete.csv")
                
            except Exception as e:
                st.error(f"Erreur lors de l'analyse par URL: {e}")
            
            st.divider()
            
            # DÃ©tail URL
            st.subheader("ğŸ” DÃ©tail d'une URL")
            url_list = df_f['url'].unique().tolist()[:100]
            url_sel = st.selectbox("SÃ©lectionner une URL", url_list)
            if url_sel:
                df_url = df_f[df_f['url'] == url_sel]
                
                c1, c2, c3, c4 = st.columns(4)
                c1.metric("Total KW", len(df_url))
                c2.metric("En perte", len(df_url[df_url['diff_pos'] < 0]))
                c3.metric("En gain", len(df_url[df_url['diff_pos'] > 0]))
                if 'volume' in df_url.columns:
                    c4.metric("Volume total", f"{int(df_url['volume'].fillna(0).sum()):,}")
                
                # Afficher les leads si dispo
                if has_leads_merged and 'leads_total' in df_url.columns:
                    # RÃ©cupÃ©rer les labels de pÃ©riode
                    p_avant = df.attrs.get('periode_avant_label', 'AVANT')
                    p_apres = df.attrs.get('periode_apres_label', 'APRÃˆS')
                    
                    c1, c2, c3, c4 = st.columns(4)
                    leads_t = df_url['leads_total'].iloc[0] if len(df_url) > 0 else 0
                    leads_av = df_url['leads_avant'].iloc[0] if len(df_url) > 0 and 'leads_avant' in df_url.columns else 0
                    leads_ap = df_url['leads_apres'].iloc[0] if len(df_url) > 0 and 'leads_apres' in df_url.columns else 0
                    leads_e = df_url['leads_evolution'].iloc[0] if len(df_url) > 0 and 'leads_evolution' in df_url.columns else 0
                    c1.metric("ğŸ“Š Leads total", f"{int(leads_t or 0):,}")
                    c2.metric(f"ğŸ“Š Leads {p_avant}", f"{int(leads_av or 0):,}")
                    c3.metric(f"ğŸ“Š Leads {p_apres}", f"{int(leads_ap or 0):,}")
                    c4.metric("ğŸ“ˆ Ã‰volution", f"{int(leads_e or 0):+,}")
                
                cols = [c for c in ['mot_cle', 'diff_pos', 'volume', 'derniere_pos', 'ancienne_pos', 'meilleure_pos'] if c in df_url.columns]
                st.dataframe(df_url[cols].sort_values('diff_pos'), use_container_width=True)
                
                # Export dÃ©tail URL
                csv_url_detail = df_url[cols].to_csv(index=False, sep=';').encode('utf-8')
                st.download_button(f"ğŸ“¥ Exporter les KW de cette URL", csv_url_detail, f"detail_url.csv")
        else:
            st.warning("Colonne 'url' non trouvÃ©e")
    
    # TAB 4: GAINS
    with tab4:
        st.header("ğŸŸ¢ Gains")
        df_gains = df_f[df_f['diff_pos'] > 0].sort_values('diff_pos', ascending=False)
        st.success(f"**{len(df_gains):,}** mots-clÃ©s en gain")
        
        cols = [c for c in ['mot_cle', 'url', 'diff_pos', 'tendance_seo', 'volume', 'derniere_pos', 'ancienne_pos'] if c in df_gains.columns]
        st.dataframe(df_gains[cols], use_container_width=True, height=600)
        
        csv_gains = df_gains[cols].to_csv(index=False, sep=';').encode('utf-8')
        st.download_button("ğŸ“¥ Exporter TOUS les gains (CSV)", csv_gains, "gains_complet.csv")
    
    # TAB 5: CANNIBALISATION
    with tab5:
        st.header("ğŸ”„ DÃ©tection de cannibalisation interne")
        st.info("**Objectif** : Identifier les KW oÃ¹ une URL perd des positions tandis qu'une autre URL du site en gagne. Avant de rÃ©optimiser une page en perte, vÃ©rifiez qu'une autre page n'a pas pris le relais !")
        
        if 'mot_cle' in df.columns and 'url' in df.columns:
            with st.spinner("Analyse des cannibalisations en cours..."):
                # Travailler sur le df complet (pas filtrÃ©) pour dÃ©tecter toutes les cannibalisations
                df_canni = df[['mot_cle', 'url', 'ancienne_pos', 'derniere_pos', 'diff_pos', 'volume']].copy()
                
                # Pour chaque KW, trouver les URLs en perte et en gain
                df_pertes_canni = df_canni[df_canni['diff_pos'] < 0].copy()
                df_gains_canni = df_canni[df_canni['diff_pos'] > 0].copy()
                
                # Trouver les KW qui ont Ã  la fois des pertes ET des gains (= cannibalisation potentielle)
                kw_en_perte = set(df_pertes_canni['mot_cle'].unique())
                kw_en_gain = set(df_gains_canni['mot_cle'].unique())
                kw_cannibalisation = kw_en_perte & kw_en_gain
                
                st.metric("ğŸ”„ KW avec cannibalisation potentielle", f"{len(kw_cannibalisation):,}")
                
                if len(kw_cannibalisation) > 0:
                    # Construire le tableau de cannibalisation
                    resultats_canni = []
                    
                    for kw in kw_cannibalisation:
                        # URLs en perte sur ce KW
                        urls_perte = df_pertes_canni[df_pertes_canni['mot_cle'] == kw].sort_values('diff_pos', ascending=True)
                        # URLs en gain sur ce KW
                        urls_gain = df_gains_canni[df_gains_canni['mot_cle'] == kw].sort_values('diff_pos', ascending=False)
                        
                        # Prendre la pire perte et le meilleur gain
                        if len(urls_perte) > 0 and len(urls_gain) > 0:
                            perte = urls_perte.iloc[0]
                            gain = urls_gain.iloc[0]
                            
                            # Volume du KW (prendre le max disponible)
                            vol = max(perte.get('volume', 0) or 0, gain.get('volume', 0) or 0)
                            
                            resultats_canni.append({
                                'mot_cle': kw,
                                'volume': vol,
                                'url_perte': perte['url'],
                                'ancienne_pos_perte': perte.get('ancienne_pos', 0),
                                'nouvelle_pos_perte': perte.get('derniere_pos', 0),
                                'diff_perte': perte.get('diff_pos', 0),
                                'url_gain': gain['url'],
                                'ancienne_pos_gain': gain.get('ancienne_pos', 0),
                                'nouvelle_pos_gain': gain.get('derniere_pos', 0),
                                'diff_gain': gain.get('diff_pos', 0),
                            })
                    
                    if resultats_canni:
                        df_resultats = pd.DataFrame(resultats_canni)
                        
                        # Trier par volume dÃ©croissant (les KW les plus importants d'abord)
                        df_resultats = df_resultats.sort_values('volume', ascending=False)
                        
                        # Filtres
                        col1, col2 = st.columns(2)
                        with col1:
                            vol_min_canni = st.number_input("Volume minimum", min_value=0, value=0, step=50, key="vol_canni")
                        with col2:
                            diff_min_canni = st.number_input("Perte minimum (positions)", min_value=0, value=0, step=1, key="diff_canni")
                        
                        # Appliquer filtres (fillna pour Ã©viter que les NaN soient exclus)
                        df_resultats_f = df_resultats[
                            (df_resultats['volume'].fillna(0) >= vol_min_canni) & 
                            (df_resultats['diff_perte'].fillna(0).abs() >= diff_min_canni)
                        ]
                        
                        st.success(f"**{len(df_resultats_f):,}** cas de cannibalisation dÃ©tectÃ©s (sur {len(df_resultats):,} total)")
                        
                        # Affichage du tableau
                        st.subheader("âš ï¸ KW Ã  risque â€” VÃ©rifier avant rÃ©optimisation")
                        
                        # Formater pour l'affichage
                        df_display = df_resultats_f.copy()
                        df_display['ğŸ“‰ URL en perte'] = df_display['url_perte']
                        df_display['Ã‰tait pos'] = df_display['ancienne_pos_perte'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['â†’ Maintenant'] = df_display['nouvelle_pos_perte'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['Diff'] = df_display['diff_perte'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['ğŸ“ˆ URL en hausse'] = df_display['url_gain']
                        df_display['Ã‰tait pos '] = df_display['ancienne_pos_gain'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['â†’ Maintenant '] = df_display['nouvelle_pos_gain'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        df_display['Diff '] = df_display['diff_gain'].apply(lambda x: f"+{int(x)}" if pd.notna(x) else "+0")
                        df_display['Volume'] = df_display['volume'].apply(lambda x: int(x) if pd.notna(x) else 0)
                        
                        cols_display = ['mot_cle', 'Volume', 'ğŸ“‰ URL en perte', 'Ã‰tait pos', 'â†’ Maintenant', 'Diff', 'ğŸ“ˆ URL en hausse', 'Ã‰tait pos ', 'â†’ Maintenant ', 'Diff ']
                        
                        st.dataframe(df_display[cols_display].head(100), use_container_width=True, height=500)
                        
                        # Alerte
                        st.warning("""
                        **âš ï¸ ATTENTION avant de rÃ©optimiser une URL en perte :**
                        1. VÃ©rifiez si l'URL en hausse rÃ©pond mieux Ã  l'intention de recherche
                        2. Si oui â†’ renforcez l'URL en hausse plutÃ´t que l'ancienne
                        3. Si non â†’ vÃ©rifiez le maillage interne pour Ã©viter la cannibalisation
                        4. Envisagez une redirection 301 si l'ancienne URL n'a plus de raison d'Ãªtre
                        """)
                        
                        # Export
                        csv_canni = df_resultats_f.to_csv(index=False, sep=';').encode('utf-8')
                        st.download_button("ğŸ“¥ Exporter les cannibalisations (CSV)", csv_canni, "cannibalisations.csv")
                        
                else:
                    st.success("âœ… Aucune cannibalisation dÃ©tectÃ©e ! Chaque KW n'a qu'une seule URL qui bouge.")
        else:
            st.warning("Colonnes 'mot_cle' et 'url' nÃ©cessaires pour l'analyse de cannibalisation")
    
    # TAB 6: SEARCH CONSOLE
    with tab6:
        st.header("ğŸ” DonnÃ©es Search Console")
        
        if has_gsc:
            st.info("**DonnÃ©es rÃ©elles Google** : Clics, impressions, CTR et positions moyennes des 12 derniers mois")
            
            # CrÃ©er les sous-onglets GSC
            gsc_tab1, gsc_tab2, gsc_tab3 = st.tabs(["ğŸš¨ URLs en danger", "ğŸ’¡ OpportunitÃ©s CTR", "ğŸ“Š Vue globale"])
            
            # === ONGLET 1 : URLs EN DANGER ===
            with gsc_tab1:
                st.subheader("ğŸš¨ URLs en danger : Perte SEO + Trafic rÃ©el")
                st.caption("URLs qui perdent des positions Haloscan ET qui ont beaucoup de clics GSC â†’ Perte de trafic rÃ©elle")
                
                if gsc_pages_df is not None and 'url' in df_f.columns:
                    # AgrÃ©ger les donnÃ©es Haloscan par URL
                    df_haloscan_urls = df_f.groupby('url').agg({
                        'diff_pos': ['mean', 'sum', 'count'],
                        'volume': 'sum'
                    }).reset_index()
                    df_haloscan_urls.columns = ['url', 'diff_pos_mean', 'diff_pos_sum', 'nb_kw', 'volume_total']
                    df_haloscan_urls['url_normalized'] = df_haloscan_urls['url'].apply(normalize_url)
                    
                    # Fusionner avec GSC
                    df_danger = df_haloscan_urls.merge(
                        gsc_pages_df[['url_normalized', 'Clics', 'Impressions', 'CTR', 'Position']],
                        on='url_normalized',
                        how='inner'
                    )
                    
                    # URLs en danger = diff nÃ©gative + beaucoup de clics
                    df_danger = df_danger[df_danger['diff_pos_mean'] < 0].copy()
                    df_danger['score_danger'] = df_danger['Clics'] * df_danger['diff_pos_mean'].abs()
                    df_danger = df_danger.sort_values('score_danger', ascending=False)
                    
                    # MÃ©triques
                    col1, col2, col3 = st.columns(3)
                    col1.metric("URLs en danger", f"{len(df_danger):,}")
                    col2.metric("Clics totaux menacÃ©s", f"{int(df_danger['Clics'].sum()):,}")
                    col3.metric("Impressions menacÃ©es", f"{int(df_danger['Impressions'].sum()):,}")
                    
                    if len(df_danger) > 0:
                        # Afficher le tableau
                        df_danger_display = df_danger[['url', 'Clics', 'Impressions', 'CTR', 'Position', 'diff_pos_mean', 'nb_kw', 'volume_total']].copy()
                        df_danger_display = df_danger_display.rename(columns={
                            'Clics': 'ğŸ–±ï¸ Clics GSC',
                            'Impressions': 'ğŸ‘ï¸ Impressions',
                            'CTR': 'ğŸ“Š CTR %',
                            'Position': 'ğŸ“ Pos GSC',
                            'diff_pos_mean': 'ğŸ“‰ Î” Haloscan',
                            'nb_kw': 'Nb KW',
                            'volume_total': 'Vol. total'
                        })
                        df_danger_display['ğŸ“‰ Î” Haloscan'] = df_danger_display['ğŸ“‰ Î” Haloscan'].round(1)
                        
                        st.dataframe(df_danger_display.head(50), use_container_width=True, height=400)
                        
                        st.error("""
                        **ğŸš¨ ACTION REQUISE** : Ces URLs perdent des positions ET gÃ©nÃ¨rent du trafic rÃ©el.
                        â†’ Prioriser leur rÃ©optimisation pour Ã©viter une perte de trafic
                        """)
                        
                        # Export
                        csv_danger = df_danger.to_csv(index=False, sep=';').encode('utf-8')
                        st.download_button("ğŸ“¥ Exporter les URLs en danger (CSV)", csv_danger, "urls_danger_gsc.csv")
                    else:
                        st.success("âœ… Aucune URL en danger dÃ©tectÃ©e !")
                else:
                    st.warning("DonnÃ©es Pages GSC ou URLs Haloscan non disponibles")
            
            # === ONGLET 2 : OPPORTUNITÃ‰S CTR ===
            with gsc_tab2:
                st.subheader("ğŸ’¡ OpportunitÃ©s CTR : Bien positionnÃ© mais peu cliquÃ©")
                st.caption("URLs en Top 10 avec CTR < 5% â†’ Title et meta description Ã  optimiser")
                
                if gsc_pages_df is not None:
                    # Filtrer : Position < 10 et CTR < 5%
                    df_ctr_opps = gsc_pages_df[
                        (gsc_pages_df['Position'] <= 10) & 
                        (gsc_pages_df['CTR'] < 5) &
                        (gsc_pages_df['Impressions'] >= 100)  # Au moins 100 impressions pour Ãªtre significatif
                    ].copy()
                    
                    # Calculer le potentiel de clics
                    # Si CTR passait Ã  5%, combien de clics en plus ?
                    df_ctr_opps['ctr_potentiel'] = 5.0
                    df_ctr_opps['clics_potentiels'] = (df_ctr_opps['Impressions'] * df_ctr_opps['ctr_potentiel'] / 100).astype(int)
                    df_ctr_opps['clics_supplementaires'] = df_ctr_opps['clics_potentiels'] - df_ctr_opps['Clics']
                    df_ctr_opps = df_ctr_opps.sort_values('clics_supplementaires', ascending=False)
                    
                    # MÃ©triques
                    col1, col2, col3 = st.columns(3)
                    col1.metric("URLs Ã  optimiser", f"{len(df_ctr_opps):,}")
                    col2.metric("Clics actuels", f"{int(df_ctr_opps['Clics'].sum()):,}")
                    col3.metric("Potentiel clics en +", f"+{int(df_ctr_opps['clics_supplementaires'].sum()):,}")
                    
                    if len(df_ctr_opps) > 0:
                        # Afficher le tableau
                        df_ctr_display = df_ctr_opps[['url', 'Position', 'CTR', 'Clics', 'Impressions', 'clics_supplementaires']].copy()
                        df_ctr_display = df_ctr_display.rename(columns={
                            'Position': 'ğŸ“ Position',
                            'CTR': 'ğŸ“Š CTR actuel %',
                            'Clics': 'ğŸ–±ï¸ Clics',
                            'Impressions': 'ğŸ‘ï¸ Impressions',
                            'clics_supplementaires': 'ğŸ¯ Potentiel clics +'
                        })
                        df_ctr_display['ğŸ“ Position'] = df_ctr_display['ğŸ“ Position'].round(1)
                        
                        st.dataframe(df_ctr_display.head(50), use_container_width=True, height=400)
                        
                        st.warning("""
                        **ğŸ’¡ OPTIMISATION RECOMMANDÃ‰E** :
                        - Revoir les **titles** pour les rendre plus attractifs
                        - AmÃ©liorer les **meta descriptions** avec des CTA
                        - Ajouter des **donnÃ©es structurÃ©es** pour enrichir les snippets
                        """)
                        
                        # Export
                        csv_ctr = df_ctr_opps.to_csv(index=False, sep=';').encode('utf-8')
                        st.download_button("ğŸ“¥ Exporter les opportunitÃ©s CTR (CSV)", csv_ctr, "opportunites_ctr.csv")
                    else:
                        st.success("âœ… Toutes les URLs en Top 10 ont un bon CTR !")
                else:
                    st.warning("DonnÃ©es Pages GSC non disponibles")
            
            # === ONGLET 3 : VUE GLOBALE ===
            with gsc_tab3:
                st.subheader("ğŸ“Š Vue globale Search Console")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**ğŸ” Top RequÃªtes (par clics)**")
                    if gsc_queries_df is not None:
                        st.dataframe(
                            gsc_queries_df[['query', 'Clics', 'Impressions', 'CTR', 'Position']].head(20),
                            use_container_width=True,
                            height=400
                        )
                    else:
                        st.info("DonnÃ©es requÃªtes non disponibles")
                
                with col2:
                    st.markdown("**ğŸ“„ Top Pages (par clics)**")
                    if gsc_pages_df is not None:
                        df_pages_display = gsc_pages_df[['url', 'Clics', 'Impressions', 'CTR', 'Position']].head(20).copy()
                        # Raccourcir les URLs pour l'affichage
                        df_pages_display['url'] = df_pages_display['url'].str.replace('https://www.ootravaux.fr', '...')
                        st.dataframe(df_pages_display, use_container_width=True, height=400)
                    else:
                        st.info("DonnÃ©es pages non disponibles")
                
                # Stats globales
                st.divider()
                st.markdown("**ğŸ“ˆ Statistiques globales GSC**")
                col1, col2, col3, col4 = st.columns(4)
                
                if gsc_pages_df is not None:
                    col1.metric("Total Clics", f"{int(gsc_pages_df['Clics'].sum()):,}")
                    col2.metric("Total Impressions", f"{int(gsc_pages_df['Impressions'].sum()):,}")
                    col3.metric("CTR moyen", f"{gsc_pages_df['CTR'].mean():.2f}%")
                    col4.metric("Position moyenne", f"{gsc_pages_df['Position'].mean():.1f}")
        
        else:
            st.warning("ğŸ‘† Uploadez un fichier ZIP Search Console pour voir les donnÃ©es de trafic rÃ©el")
            st.info("""
            **Comment obtenir l'export :**
            1. Allez sur [Google Search Console](https://search.google.com/search-console)
            2. SÃ©lectionnez votre propriÃ©tÃ©
            3. Allez dans "Performances" > "RÃ©sultats de recherche"
            4. Cliquez sur "Exporter" > "TÃ©lÃ©charger au format ZIP"
            """)
    
    # TAB 7: RAPPORT
    with tab7:
        st.header("ğŸ“ Rapport complet pour l'Ã©quipe Ã©dito")
        
        if st.button("ğŸ”„ GÃ©nÃ©rer le rapport complet", type="primary"):
            
            # Calculs pour le rapport
            df_pertes_rapport = df_f[df_f['diff_pos'] < 0].sort_values('diff_pos', ascending=True)
            df_gains_rapport = df_f[df_f['diff_pos'] > 0].sort_values('diff_pos', ascending=False)
            
            # URLs les plus impactÃ©es
            urls_critiques = pd.DataFrame()  # Initialiser vide par dÃ©faut
            if 'url' in df_f.columns and len(df_pertes_rapport) > 0:
                # Construire l'agrÃ©gation dynamiquement selon les colonnes disponibles
                agg_url = {'diff_pos': 'count'}
                if 'volume' in df_pertes_rapport.columns:
                    agg_url['volume'] = 'sum'
                if has_leads_merged:
                    if 'leads_total' in df_pertes_rapport.columns:
                        agg_url['leads_total'] = 'first'
                    if 'leads_avant' in df_pertes_rapport.columns:
                        agg_url['leads_avant'] = 'first'
                    if 'leads_apres' in df_pertes_rapport.columns:
                        agg_url['leads_apres'] = 'first'
                    if 'leads_evolution' in df_pertes_rapport.columns:
                        agg_url['leads_evolution'] = 'first'
                    if 'tendance_leads' in df_pertes_rapport.columns:
                        agg_url['tendance_leads'] = 'first'
                
                try:
                    urls_critiques = df_pertes_rapport.groupby('url').agg(agg_url).reset_index()
                    
                    # Renommer les colonnes
                    rename_cols = {'diff_pos': 'nb_kw_perdus', 'volume': 'volume_impacte'}
                    urls_critiques = urls_critiques.rename(columns=rename_cols)
                    
                    # Trier par Ã©volution leads ou par nb KW perdus
                    if 'leads_evolution' in urls_critiques.columns:
                        urls_critiques = urls_critiques.sort_values('leads_evolution', ascending=True)
                    else:
                        urls_critiques = urls_critiques.sort_values('nb_kw_perdus', ascending=False)
                except Exception as e:
                    st.warning(f"Erreur agrÃ©gation URLs: {e}")
                    urls_critiques = pd.DataFrame()
            
            # Calcul impact leads - ATTENTION : Ã©viter de compter plusieurs fois la mÃªme URL
            if has_leads_merged:
                # Grouper par URL pour ne compter qu'une fois les leads de chaque URL
                urls_en_perte = df_pertes_rapport['url'].unique() if 'url' in df_pertes_rapport.columns else []
                df_urls_perte_unique = df_pertes_rapport.drop_duplicates(subset=['url'])
                
                total_leads_perte = int(df_urls_perte_unique['leads_total'].fillna(0).sum())
                total_leads_avant_perte = int(df_urls_perte_unique['leads_avant'].fillna(0).sum())
                total_leads_apres_perte = int(df_urls_perte_unique['leads_apres'].fillna(0).sum())
                leads_evolution_total = int(df_urls_perte_unique['leads_evolution'].fillna(0).sum())
            
            # DÃ©finir la pÃ©riode pour le titre du rapport
            if has_dual_haloscan:
                periode_rapport = f"{label_debut_p1} â†’ {label_fin_p2}"
            else:
                periode_rapport = "PÃ©riode analysÃ©e"
            
            report = f"""# ğŸ“Š RAPPORT D'ANALYSE SEO COMPLET
## PÃ©riode : {periode_rapport}
## GÃ©nÃ©rÃ© le {datetime.now().strftime('%d/%m/%Y Ã  %H:%M')}

---

# 1. SYNTHÃˆSE GLOBALE

| MÃ©trique | Valeur |
|----------|--------|
| **Total mots-clÃ©s analysÃ©s** | {total:,} |
| **Mots-clÃ©s en perte** | {pertes:,} ({pertes/total*100:.1f}%) |
| **Mots-clÃ©s en gain** | {gains:,} ({gains/total*100:.1f}%) |
| **Mots-clÃ©s stables** | {stables:,} ({stables/total*100:.1f}%) |
| **Volume de recherche perdu** | {vol_perdu:,} /mois |
| **Volume de recherche gagnÃ©** | {vol_gagne:,} /mois |
| **Bilan net volume** | {vol_gagne - vol_perdu:+,} /mois |
"""
            
            if has_leads_merged:
                periodes_info = f"PÃ©riode AVANT: {', '.join(periode_avant) if periode_avant else 'N/A'} | PÃ©riode APRÃˆS: {', '.join(periode_apres) if periode_apres else 'N/A'}"
                report += f"""
## ğŸ’° IMPACT BUSINESS (Leads)

**{periodes_info}**

| MÃ©trique | Valeur |
|----------|--------|
| **Leads historiques sur URLs en perte** | {total_leads_perte:,} |
| **Leads pÃ©riode AVANT** | {total_leads_avant_perte:,} |
| **Leads pÃ©riode APRÃˆS** | {total_leads_apres_perte:,} |
| **Ã‰volution des leads** | {leads_evolution_total:+,} |

âš ï¸ **Ces URLs gÃ©nÃ¨rent des leads et perdent en visibilitÃ© SEO = PRIORITÃ‰ MAXIMALE**

"""

            # Section multi-pÃ©riodes si disponible
            if has_dual_haloscan and 'tendance_multi' in df_f.columns:
                tendances = df_f['tendance_multi'].value_counts()
                chute_continue = tendances.get('ğŸ“‰ğŸ“‰ Chute continue', 0)
                rebond_rechute = tendances.get('ğŸ“ˆğŸ“‰ Rebond puis rechute', 0)
                recuperation = tendances.get('ğŸ“‰ğŸ“ˆ RÃ©cupÃ©ration', 0)
                hausse_continue = tendances.get('ğŸ“ˆğŸ“ˆ Hausse continue', 0)
                
                report += f"""---

## ğŸ“ˆ ANALYSE MULTI-PÃ‰RIODES ({label_debut_p1} â†’ {label_fin_p1} â†’ {label_fin_p2})

| Tendance | Nombre de KW | Signification |
|----------|--------------|---------------|
| ğŸ“‰ğŸ“‰ **Chute continue** | {chute_continue:,} | Perte sur P1 ET P2 â€” **ProblÃ¨me structurel** |
| ğŸ“ˆğŸ“‰ Rebond puis rechute | {rebond_rechute:,} | Gain sur P1 puis perte sur P2 |
| ğŸ“‰ğŸ“ˆ RÃ©cupÃ©ration | {recuperation:,} | Perte sur P1 puis gain sur P2 |
| ğŸ“ˆğŸ“ˆ Hausse continue | {hausse_continue:,} | Gain sur P1 ET P2 |

"""
                # Ajouter les KW en chute continue (TOP 100)
                df_chute_continue = df_f[df_f['tendance_multi'] == 'ğŸ“‰ğŸ“‰ Chute continue'].copy()
                if len(df_chute_continue) > 0:
                    report += f"""### ğŸš¨ TOP 100 KW en CHUTE CONTINUE â€” PrioritÃ© maximale

| Mot-clÃ© | URL | Pos {label_debut_p1} | Pos {label_fin_p1} | Î” P1 | Pos {label_fin_p2} | Î” P2 | Î” TOTAL | Volume |
|---------|-----|---------------------|--------------------|----- |--------------------|----- |---------|--------|
"""
                    # Trier par diff totale
                    df_chute_continue = df_chute_continue.sort_values('diff_pos', ascending=True)
                    
                    for _, row in df_chute_continue.head(100).iterrows():
                        mc = str(row.get('mot_cle', 'N/A'))[:40]
                        url = str(row.get('url', 'N/A'))
                        pos_debut = int(row.get('pos_debut_p1', 0)) if pd.notna(row.get('pos_debut_p1')) else 0
                        pos_mid = int(row.get('pos_fin_p1', 0)) if pd.notna(row.get('pos_fin_p1')) else 0
                        diff_p1 = int(row.get('diff_p1', 0)) if pd.notna(row.get('diff_p1')) else 0
                        pos_fin = int(row.get('pos_fin_p2', 0)) if pd.notna(row.get('pos_fin_p2')) else 0
                        diff_p2 = int(row.get('diff_p2', 0)) if pd.notna(row.get('diff_p2')) else 0
                        diff_tot = int(row.get('diff_pos', 0)) if pd.notna(row.get('diff_pos')) else 0
                        vol = int(row.get('volume', 0)) if pd.notna(row.get('volume')) else 0
                        report += f"| {mc} | {url} | {pos_debut} | {pos_mid} | {diff_p1} | {pos_fin} | {diff_p2} | {diff_tot} | {vol:,} |\n"
                    
                    if len(df_chute_continue) > 100:
                        report += f"\n_+ {len(df_chute_continue) - 100:,} autres KW en chute continue (non affichÃ©s)_\n"

            # Section Search Console si disponible
            if has_gsc and gsc_pages_df is not None and 'url' in df_f.columns:
                # Calculer les URLs en danger
                df_haloscan_urls_rpt = df_f.groupby('url').agg({
                    'diff_pos': 'mean',
                    'volume': 'sum'
                }).reset_index()
                df_haloscan_urls_rpt['url_normalized'] = df_haloscan_urls_rpt['url'].apply(normalize_url)
                
                df_danger_rpt = df_haloscan_urls_rpt.merge(
                    gsc_pages_df[['url_normalized', 'Clics', 'Impressions', 'CTR', 'Position']],
                    on='url_normalized',
                    how='inner'
                )
                df_danger_rpt = df_danger_rpt[df_danger_rpt['diff_pos'] < 0].copy()
                df_danger_rpt = df_danger_rpt.sort_values('Clics', ascending=False)
                
                # OpportunitÃ©s CTR
                df_ctr_rpt = gsc_pages_df[
                    (gsc_pages_df['Position'] <= 10) & 
                    (gsc_pages_df['CTR'] < 5) &
                    (gsc_pages_df['Impressions'] >= 100)
                ].copy()
                df_ctr_rpt['clics_potentiels'] = (df_ctr_rpt['Impressions'] * 5 / 100 - df_ctr_rpt['Clics']).astype(int)
                df_ctr_rpt = df_ctr_rpt.sort_values('clics_potentiels', ascending=False)
                
                report += f"""---

## ğŸ” DONNÃ‰ES SEARCH CONSOLE (12 derniers mois)

| MÃ©trique | Valeur |
|----------|--------|
| **Total clics** | {int(gsc_pages_df['Clics'].sum()):,} |
| **Total impressions** | {int(gsc_pages_df['Impressions'].sum()):,} |
| **CTR moyen** | {gsc_pages_df['CTR'].mean():.2f}% |
| **URLs en danger (perte + clics)** | {len(df_danger_rpt):,} |
| **OpportunitÃ©s CTR (Top 10, CTR < 5%)** | {len(df_ctr_rpt):,} |
| **Clics potentiels Ã  gagner** | +{int(df_ctr_rpt['clics_potentiels'].sum()):,} |

"""
                if len(df_danger_rpt) > 0:
                    report += """### ğŸš¨ TOP 20 URLs EN DANGER (Perte SEO + Trafic rÃ©el)

| URL | Clics GSC | Î” Haloscan | Position GSC | CTR |
|-----|-----------|------------|--------------|-----|
"""
                    for _, row in df_danger_rpt.head(20).iterrows():
                        url = str(row.get('url', 'N/A'))
                        clics = int(row.get('Clics', 0))
                        diff = round(row.get('diff_pos', 0), 1)
                        pos = round(row.get('Position', 0), 1)
                        ctr = round(row.get('CTR', 0), 2)
                        report += f"| {url} | {clics:,} | {diff} | {pos} | {ctr}% |\n"
                
                if len(df_ctr_rpt) > 0:
                    report += """

### ğŸ’¡ TOP 20 OPPORTUNITÃ‰S CTR (Ã  optimiser)

| URL | Position | CTR actuel | Impressions | Potentiel clics + |
|-----|----------|------------|-------------|-------------------|
"""
                    for _, row in df_ctr_rpt.head(20).iterrows():
                        url = str(row.get('url', 'N/A'))
                        pos = round(row.get('Position', 0), 1)
                        ctr = round(row.get('CTR', 0), 2)
                        impr = int(row.get('Impressions', 0))
                        pot = int(row.get('clics_potentiels', 0))
                        report += f"| {url} | {pos} | {ctr}% | {impr:,} | +{pot:,} |\n"

            report += """---

# 2. DIAGNOSTIC

"""
            if gains == 0:
                report += f"""âš ï¸ **SITUATION CRITIQUE** : Le site n'a aucun gain de position.
- {pertes:,} mots-clÃ©s en perte
- Action recommandÃ©e : **Audit urgent des contenus**

"""
            elif pertes > gains:
                report += f"""âš ï¸ **SITUATION PRÃ‰OCCUPANTE** : Le site perd plus de positions qu'il n'en gagne.
- Ratio pertes/gains : {pertes/gains:.1f}x plus de pertes
- Action recommandÃ©e : **Audit urgent des contenus impactÃ©s**

"""
            elif pertes == 0:
                report += f"""âœ… **SITUATION EXCELLENTE** : Aucune perte de position !
- {gains:,} mots-clÃ©s en gain

"""
            else:
                report += f"""âœ… **SITUATION POSITIVE** : Le site gagne plus de positions qu'il n'en perd.
- Ratio gains/pertes : {gains/pertes:.1f}x plus de gains

"""

            if len(urls_critiques) > 0:
                report += f"""---

# 3. TOUTES LES PAGES Ã€ TRAITER ({len(urls_critiques):,} URLs)

"""
                if has_leads_merged:
                    # RÃ©cupÃ©rer les labels de pÃ©riode
                    p_avant = df.attrs.get('periode_avant_label', 'AVANT')
                    p_apres = df.attrs.get('periode_apres_label', 'APRÃˆS')
                    
                    report += f"""**TriÃ©es par Ã©volution des leads** â€” Les URLs avec les plus grosses pertes de leads en premier.

| PrioritÃ© | URL | KW perdus | Volume | Leads {p_avant} | Leads {p_apres} | ğŸ“Š TENDANCE |
|----------|-----|-----------|--------|-------------|-------------|-------------|
"""
                    for i, row in urls_critiques.iterrows():
                        leads_evol = row.get('leads_evolution', 0)
                        leads_evol = 0 if pd.isna(leads_evol) else leads_evol
                        tendance = row.get('tendance_leads', 'â¡ï¸ N/A')
                        prio = "ğŸ”´ CRITIQUE" if leads_evol < -100 else \
                               "ğŸŸ  URGENT" if leads_evol < -20 else \
                               "ğŸŸ¡ MOYEN" if leads_evol < 0 else "âšª STABLE/HAUSSE"
                        
                        # SÃ©curiser toutes les valeurs numÃ©riques
                        nb_kw = int(row.get('nb_kw_perdus', 0) or 0)
                        vol = row.get('volume_impacte', 0)
                        vol = 0 if pd.isna(vol) else int(vol)
                        l_avant = row.get('leads_avant', 0)
                        l_avant = 0 if pd.isna(l_avant) else int(l_avant)
                        l_apres = row.get('leads_apres', 0)
                        l_apres = 0 if pd.isna(l_apres) else int(l_apres)
                        
                        report += f"| {prio} | {row['url']} | {nb_kw} | {vol:,} | {l_avant:,} | {l_apres:,} | {tendance} |\n"
                else:
                    report += """**TriÃ©es par nombre de mots-clÃ©s perdus**

| PrioritÃ© | URL | KW perdus | Volume impactÃ© |
|----------|-----|-----------|----------------|
"""
                    for i, row in urls_critiques.iterrows():
                        nb_kw = row['nb_kw_perdus']
                        prio = "ğŸ”´ URGENT" if nb_kw > 50 else "ğŸŸ  MOYEN" if nb_kw > 10 else "ğŸŸ¡ FAIBLE"
                        report += f"| {prio} | {row['url']} | {int(nb_kw)} | {int(row.get('volume_impacte', 0) or 0):,} |\n"
            else:
                report += """---

# 3. PAGES Ã€ TRAITER

_Aucune URL en perte dÃ©tectÃ©e_

"""

            # Filtrer les KW qui ont vraiment morflÃ© (grosses pertes uniquement)
            # PrioritÃ© : diff trÃ¨s nÃ©gative + volume Ã©levÃ©
            df_pertes_critiques = df_pertes_rapport[df_pertes_rapport['diff_pos'] <= -5].copy()
            
            # Grouper par URL et garder le KW principal :
            # = celui avec le plus gros volume PARMI ceux oÃ¹ l'URL rankait bien avant (ancienne_pos â‰¤ 10)
            if 'volume' in df_pertes_critiques.columns and 'url' in df_pertes_critiques.columns:
                # Filtrer les KW oÃ¹ l'URL rankait vraiment bien (top 10)
                df_bien_ranke = df_pertes_critiques[df_pertes_critiques['ancienne_pos'] <= 10].copy()
                
                # Si pas de KW bien rankÃ© pour une URL, on prend quand mÃªme le meilleur volume
                if len(df_bien_ranke) > 0:
                    idx_kw_principal = df_bien_ranke.groupby('url')['volume'].idxmax()
                    df_pertes_par_url = df_bien_ranke.loc[idx_kw_principal].copy()
                else:
                    idx_kw_principal = df_pertes_critiques.groupby('url')['volume'].idxmax()
                    df_pertes_par_url = df_pertes_critiques.loc[idx_kw_principal].copy()
                
                # Ajouter le nombre total de KW perdus par URL (tous les KW, pas que les bien rankÃ©s)
                kw_count = df_pertes_critiques.groupby('url').size().rename('nb_kw_perdus')
                df_pertes_par_url = df_pertes_par_url.merge(kw_count, on='url', how='left')
                
                # Trier par diff_pos (les pires en premier)
                df_pertes_par_url = df_pertes_par_url.sort_values('diff_pos', ascending=True)
            else:
                df_pertes_par_url = df_pertes_critiques.sort_values('diff_pos', ascending=True)
            
            # Limiter Ã  500 URLs max
            max_kw_rapport = 500
            df_pertes_limited = df_pertes_par_url.head(max_kw_rapport)
            
            report += f"""

---

# 4. PERTES CRITIQUES â€” TOP {len(df_pertes_limited):,} URLs (pertes â‰¥ 5 positions)

**âš ï¸ PrioritÃ© maximale â€” KW principal = plus gros volume parmi ceux oÃ¹ l'URL Ã©tait en top 10**

| KW Principal | URL | Ancienne pos | Nouvelle pos | Diff | Volume | Nb KW perdus |
|--------------|-----|--------------|--------------|------|--------|--------------|
"""
            for _, row in df_pertes_limited.iterrows():
                mc = str(row.get('mot_cle', 'N/A'))[:50]
                url = str(row.get('url', 'N/A'))
                anc = row.get('ancienne_pos', 0)
                anc = 0 if pd.isna(anc) else int(anc)
                dern = row.get('derniere_pos', 0)
                dern = 0 if pd.isna(dern) else int(dern)
                diff = row.get('diff_pos', 0)
                diff = 0 if pd.isna(diff) else int(diff)
                vol = row.get('volume', 0)
                vol = 0 if pd.isna(vol) else int(vol)
                nb_kw = row.get('nb_kw_perdus', 1)
                nb_kw = 1 if pd.isna(nb_kw) else int(nb_kw)
                report += f"| {mc} | {url} | {anc} | {dern} | {diff} | {vol:,} | {nb_kw} |\n"
            
            # Info sur les URLs non affichÃ©es
            nb_autres_urls = len(df_pertes_par_url) - len(df_pertes_limited)
            if nb_autres_urls > 0:
                report += f"\n_+ {nb_autres_urls:,} autres URLs avec des pertes â‰¥ 5 positions (non affichÃ©es)_\n"

            # Filtrer les KW avec gains significatifs (â‰¥ 5 positions)
            df_gains_significatifs = df_gains_rapport[df_gains_rapport['diff_pos'] >= 5].copy()
            
            # Grouper par URL et garder le KW principal :
            # = celui avec le plus gros volume PARMI ceux oÃ¹ l'URL ranke bien maintenant (derniere_pos â‰¤ 10)
            if 'volume' in df_gains_significatifs.columns and 'url' in df_gains_significatifs.columns and len(df_gains_significatifs) > 0:
                # Filtrer les KW oÃ¹ l'URL ranke vraiment bien maintenant (top 10)
                df_bien_ranke = df_gains_significatifs[df_gains_significatifs['derniere_pos'] <= 10].copy()
                
                if len(df_bien_ranke) > 0:
                    idx_kw_principal = df_bien_ranke.groupby('url')['volume'].idxmax()
                    df_gains_par_url = df_bien_ranke.loc[idx_kw_principal].copy()
                else:
                    idx_kw_principal = df_gains_significatifs.groupby('url')['volume'].idxmax()
                    df_gains_par_url = df_gains_significatifs.loc[idx_kw_principal].copy()
                
                kw_count = df_gains_significatifs.groupby('url').size().rename('nb_kw_gains')
                df_gains_par_url = df_gains_par_url.merge(kw_count, on='url', how='left')
                
                df_gains_par_url = df_gains_par_url.sort_values('diff_pos', ascending=False)
            else:
                df_gains_par_url = df_gains_significatifs.sort_values('diff_pos', ascending=False)
            
            df_gains_limited = df_gains_par_url.head(max_kw_rapport)
            
            report += f"""

---

# 5. GAINS SIGNIFICATIFS â€” TOP {len(df_gains_limited):,} URLs (gains â‰¥ 5 positions)

**âœ… Ce qui fonctionne â€” KW principal = plus gros volume parmi ceux en top 10 actuel**

| KW Principal | URL | Ancienne pos | Nouvelle pos | Diff | Volume | Nb KW gagnÃ©s |
|--------------|-----|--------------|--------------|------|--------|--------------|
"""
            for _, row in df_gains_limited.iterrows():
                mc = str(row.get('mot_cle', 'N/A'))[:50]
                url = str(row.get('url', 'N/A'))
                anc = row.get('ancienne_pos', 0)
                anc = 0 if pd.isna(anc) else int(anc)
                dern = row.get('derniere_pos', 0)
                dern = 0 if pd.isna(dern) else int(dern)
                diff = row.get('diff_pos', 0)
                diff = 0 if pd.isna(diff) else int(diff)
                vol = row.get('volume', 0)
                vol = 0 if pd.isna(vol) else int(vol)
                nb_kw = row.get('nb_kw_gains', 1)
                nb_kw = 1 if pd.isna(nb_kw) else int(nb_kw)
                report += f"| {mc} | {url} | {anc} | {dern} | +{diff} | {vol:,} | {nb_kw} |\n"
            
            nb_autres_urls = len(df_gains_par_url) - len(df_gains_limited)
            if nb_autres_urls > 0:
                report += f"\n_+ {nb_autres_urls:,} autres URLs avec des gains â‰¥ 5 positions (non affichÃ©es)_\n"

            report += f"""

---

# 6. RECOMMANDATIONS POUR L'Ã‰QUIPE Ã‰DITO

## ğŸ”´ Actions immÃ©diates (cette semaine)
"""
            if has_leads_merged:
                report += """1. **PRIORITÃ‰ ABSOLUE : URLs avec leads + pertes SEO** â€” Ces pages gÃ©nÃ¨rent du business ET perdent en visibilitÃ©
2. **Auditer le contenu** des 10 premiÃ¨res URLs critiques
3. **VÃ©rifier le maillage interne** vers ces pages stratÃ©giques
"""
            else:
                report += """1. **Auditer les 10 premiÃ¨res URLs critiques** â€” VÃ©rifier : contenu Ã  jour ? maillage interne ? balises optimisÃ©es ?
2. **Identifier les KW Ã  fort volume perdus** â€” Filtrer les pertes avec volume > 1000
3. **VÃ©rifier la concurrence** â€” Les concurrents ont-ils amÃ©liorÃ© leur contenu ?
"""

            report += """
## ğŸŸ  Actions court terme (ce mois)
1. **Mettre Ã  jour les contenus des pages critiques** â€” Enrichir, actualiser, ajouter des sections
2. **Renforcer le maillage interne** vers les pages en perte
3. **CrÃ©er du contenu de support** pour les thÃ©matiques en baisse

## ğŸŸ¡ Actions moyen terme (ce trimestre)
1. **Audit technique** â€” VÃ©rifier Core Web Vitals des pages impactÃ©es
2. **Analyse des backlinks** â€” Les pages ont-elles perdu des liens ?
3. **StratÃ©gie de contenu** â€” Planifier les mises Ã  jour rÃ©currentes

---

# 7. MÃ‰TRIQUES DE SUIVI

Refaire cette analyse dans 1 mois pour mesurer :
- [ ] RÃ©duction du nombre de KW en perte
- [ ] RÃ©cupÃ©ration des positions sur les KW prioritaires
- [ ] AmÃ©lioration du volume de recherche captÃ©
"""
            if has_leads_merged:
                report += """- [ ] Stabilisation ou hausse des leads sur les URLs retravaillÃ©es
"""

            report += f"""
---

_Rapport gÃ©nÃ©rÃ© automatiquement â€” Haloscan SEO Diff Analyzer_
_DonnÃ©es : {len(df):,} mots-clÃ©s analysÃ©s"""
            
            if has_leads_merged:
                report += f" | {len(leads_df):,} URLs avec donnÃ©es leads"
            
            report += "_\n"
            
            st.session_state['report'] = report
            st.success("âœ… Rapport gÃ©nÃ©rÃ© !")
        
        if 'report' in st.session_state:
            st.markdown(st.session_state['report'])
            
            st.divider()
            
            # === ANALYSE IA ===
            st.subheader("ğŸ¤– Analyse IA et TODO")
            
            if anthropic_api_key:
                if st.button("ğŸ¤– GÃ©nÃ©rer l'analyse IA", type="secondary"):
                    with st.spinner("Claude Opus 4.5 analyse vos donnÃ©es... (peut prendre 30-60 secondes)"):
                        try:
                            import anthropic
                            
                            client = anthropic.Anthropic(api_key=anthropic_api_key)
                            
                            # PrÃ©parer les donnÃ©es pour le LLM
                            # 1. MÃ©triques globales
                            metrics_globales = {
                                "total_kw": total,
                                "kw_en_perte": pertes,
                                "kw_en_gain": gains,
                                "kw_stables": stables,
                                "volume_perdu": vol_perdu,
                                "volume_gagne": vol_gagne,
                                "bilan_volume": vol_gagne - vol_perdu
                            }
                            
                            # 2. Top 50 URLs critiques (en perte)
                            df_pertes_ia = df_f[df_f['diff_pos'] < 0].copy()
                            if 'url' in df_pertes_ia.columns:
                                urls_critiques_ia = df_pertes_ia.groupby('url').agg({
                                    'diff_pos': ['count', 'mean'],
                                    'volume': 'sum'
                                }).reset_index()
                                urls_critiques_ia.columns = ['url', 'nb_kw_perdus', 'diff_moyenne', 'volume_total']
                                
                                # Ajouter leads si disponible
                                if has_leads_merged:
                                    leads_by_url = df_pertes_ia.groupby('url').agg({
                                        'leads_total': 'first',
                                        'leads_evolution': 'first'
                                    }).reset_index()
                                    urls_critiques_ia = urls_critiques_ia.merge(leads_by_url, on='url', how='left')
                                
                                urls_critiques_ia = urls_critiques_ia.sort_values('volume_total', ascending=False).head(50)
                                urls_critiques_list = urls_critiques_ia.to_dict('records')
                            else:
                                urls_critiques_list = []
                            
                            # 3. Top 30 KW en perte (les plus impactants)
                            top_kw_pertes = df_pertes_ia.nlargest(30, 'volume')[['mot_cle', 'url', 'diff_pos', 'volume', 'ancienne_pos', 'derniere_pos']].to_dict('records') if 'volume' in df_pertes_ia.columns else []
                            
                            # 4. DonnÃ©es multi-pÃ©riodes si disponibles
                            tendances_multi = {}
                            if has_dual_haloscan and 'tendance_multi' in df_f.columns:
                                tendances_multi = df_f['tendance_multi'].value_counts().to_dict()
                                # Top 20 en chute continue
                                df_chute = df_f[df_f['tendance_multi'] == 'ğŸ“‰ğŸ“‰ Chute continue'].head(20)
                                if len(df_chute) > 0:
                                    tendances_multi['top_chute_continue'] = df_chute[['mot_cle', 'url', 'diff_pos', 'volume']].to_dict('records')
                            
                            # 5. DonnÃ©es GSC si disponibles
                            gsc_data = {}
                            if has_gsc and gsc_pages_df is not None:
                                gsc_data['total_clics'] = int(gsc_pages_df['Clics'].sum())
                                gsc_data['total_impressions'] = int(gsc_pages_df['Impressions'].sum())
                                gsc_data['ctr_moyen'] = round(gsc_pages_df['CTR'].mean(), 2)
                                # Top 20 pages par clics
                                gsc_data['top_pages'] = gsc_pages_df.nlargest(20, 'Clics')[['url', 'Clics', 'Impressions', 'CTR', 'Position']].to_dict('records')
                            
                            # 6. Cannibalisations
                            cannibalisations = []
                            if 'mot_cle' in df_f.columns:
                                df_canni_ia = df_f[['mot_cle', 'url', 'diff_pos', 'volume']].copy()
                                df_pertes_c = df_canni_ia[df_canni_ia['diff_pos'] < 0]
                                df_gains_c = df_canni_ia[df_canni_ia['diff_pos'] > 0]
                                kw_perte = set(df_pertes_c['mot_cle'].unique())
                                kw_gain = set(df_gains_c['mot_cle'].unique())
                                kw_canni = kw_perte & kw_gain
                                if len(kw_canni) > 0:
                                    for kw in list(kw_canni)[:20]:
                                        url_perte = df_pertes_c[df_pertes_c['mot_cle'] == kw].iloc[0]['url'] if len(df_pertes_c[df_pertes_c['mot_cle'] == kw]) > 0 else None
                                        url_gain = df_gains_c[df_gains_c['mot_cle'] == kw].iloc[0]['url'] if len(df_gains_c[df_gains_c['mot_cle'] == kw]) > 0 else None
                                        if url_perte and url_gain:
                                            cannibalisations.append({'mot_cle': kw, 'url_perte': url_perte, 'url_gain': url_gain})
                            
                            # Construire le contexte JSON
                            context_data = {
                                "metriques_globales": metrics_globales,
                                "urls_critiques": urls_critiques_list,
                                "top_kw_en_perte": top_kw_pertes,
                                "tendances_multi_periodes": tendances_multi,
                                "donnees_gsc": gsc_data,
                                "cannibalisations_detectees": cannibalisations,
                                "has_leads": has_leads_merged,
                                "has_gsc": has_gsc,
                                "has_dual_period": has_dual_haloscan
                            }
                            
                            # Prompt systÃ¨me
                            system_prompt = """Tu es un expert SEO senior spÃ©cialisÃ© dans l'analyse de donnÃ©es et la stratÃ©gie de contenu.

Tu reÃ§ois des donnÃ©es SEO complÃ¨tes d'un site et tu dois produire :

1. **ANALYSE STRATÃ‰GIQUE** (5-10 lignes max)
- Diagnostic clair et direct de la situation
- Identification des patterns (saisonnalitÃ©, problÃ¨me technique, cannibalisation...)
- Points d'alerte majeurs

2. **TODO POUR L'Ã‰QUIPE CONTENT** 
Une liste d'actions CONCRÃˆTES et PRIORISÃ‰ES. Chaque action doit Ãªtre :
- PrÃ©cise (pas de "amÃ©liorer le contenu" mais "ajouter une section FAQ avec les questions X, Y, Z")
- Assignable (une personne peut la prendre et la faire)
- Avec l'URL exacte concernÃ©e
- Avec l'impact attendu (estimation)

Format de la TODO :
```
## ğŸ”´ PRIORITÃ‰ HAUTE (Ã  faire cette semaine)
- [ ] **[Action prÃ©cise]** - URL: [url complÃ¨te] - Impact: [estimation] - Raison: [pourquoi]

## ğŸŸ  PRIORITÃ‰ MOYENNE (Ã  faire ce mois)
- [ ] **[Action prÃ©cise]** - URL: [url complÃ¨te] - Impact: [estimation] - Raison: [pourquoi]

## ğŸŸ¡ PRIORITÃ‰ BASSE (Ã  planifier)
- [ ] **[Action prÃ©cise]** - URL: [url complÃ¨te] - Impact: [estimation] - Raison: [pourquoi]
```

3. **ALERTES** (si applicable)
- Risques identifiÃ©s
- DÃ©pendances dangereuses
- Tendances inquiÃ©tantes

Sois direct, pragmatique, et orientÃ© action. Pas de blabla corporate. L'Ã©quipe content doit pouvoir prendre cette TODO et l'exÃ©cuter immÃ©diatement."""

                            # Appel Ã  Claude
                            message = client.messages.create(
                                model="claude-opus-4-5-20251101",
                                max_tokens=4096,
                                system=system_prompt,
                                messages=[
                                    {
                                        "role": "user",
                                        "content": f"""Voici les donnÃ©es SEO Ã  analyser :

```json
{json.dumps(context_data, ensure_ascii=False, indent=2, default=str)}
```

GÃ©nÃ¨re ton analyse stratÃ©gique et la TODO priorisÃ©e pour l'Ã©quipe content."""
                                    }
                                ]
                            )
                            
                            # Extraire la rÃ©ponse
                            ai_analysis = message.content[0].text
                            
                            # Stocker dans session_state
                            st.session_state['ai_analysis'] = ai_analysis
                            
                            st.success("âœ… Analyse IA gÃ©nÃ©rÃ©e !")
                            
                        except anthropic.AuthenticationError:
                            st.error("âŒ ClÃ© API invalide. VÃ©rifiez votre clÃ© Anthropic.")
                        except Exception as e:
                            st.error(f"âŒ Erreur lors de l'analyse IA : {str(e)}")
                
                # Afficher l'analyse IA si disponible
                if 'ai_analysis' in st.session_state:
                    st.divider()
                    st.markdown("## ğŸ¤– Analyse IA et TODO")
                    st.markdown(st.session_state['ai_analysis'])
                    
                    # Bouton pour tÃ©lÃ©charger le rapport complet (avec IA)
                    rapport_complet = st.session_state['report'] + "\n\n---\n\n# ğŸ¤– ANALYSE IA ET TODO\n\n" + st.session_state['ai_analysis']
                    st.download_button(
                        "ğŸ“¥ TÃ©lÃ©charger le rapport COMPLET avec IA (Markdown)",
                        rapport_complet,
                        "rapport_seo_complet_avec_ia.md",
                        "text/markdown"
                    )
            else:
                st.info("ğŸ‘† Entrez votre clÃ© API Anthropic dans la sidebar pour activer l'analyse IA")
            
            st.divider()
            
            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    "ğŸ“¥ TÃ©lÃ©charger le rapport (Markdown)", 
                    st.session_state['report'], 
                    "rapport_seo_complet.md",
                    "text/markdown"
                )
            with col2:
                # Export aussi en CSV les donnÃ©es brutes
                df_export = df_f[df_f['diff_pos'] < 0].sort_values('diff_pos', ascending=True)
                cols_export = [c for c in ['mot_cle', 'url', 'ancienne_pos', 'derniere_pos', 'diff_pos', 'tendance_seo', 'volume', 'leads_total', 'leads_avant', 'leads_apres', 'leads_evolution', 'tendance_leads'] if c in df_export.columns]
                csv_export = df_export[cols_export].to_csv(index=False, sep=';').encode('utf-8')
                st.download_button(
                    "ğŸ“¥ TÃ©lÃ©charger les donnÃ©es (CSV)",
                    csv_export,
                    "pertes_completes.csv",
                    "text/csv"
                )

else:
    st.info("ğŸ‘† Charge un fichier CSV pour commencer")
